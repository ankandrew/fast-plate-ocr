{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fast &amp; Lightweight License Plate OCR","text":"<p><code>fast-plate-ocr</code> is a lightweight and fast OCR framework for license plate text recognition. You can train models from scratch or use the trained models for inference.</p> <p>The idea is to use this after a plate object detector, since the OCR expects the cropped plates.</p> <p>\ud83d\ude80 Try it on Hugging Face Spaces!</p> <p>You can try <code>fast-plate-ocr</code> pre-trained models in Hugging Spaces. No setup required!</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Keras 3 Backend Support: Train seamlessly using TensorFlow, JAX, or PyTorch backends \ud83e\udde0</li> <li>Augmentation Variety: Diverse training-time augmentations via Albumentations library \ud83d\uddbc\ufe0f</li> <li>Efficient Execution: Lightweight models that are cheap to run \ud83d\udcb0</li> <li>ONNX Runtime Inference: Fast and optimized inference with ONNX runtime \u26a1</li> <li>User-Friendly CLI: Simplified CLI for training and validating OCR models \ud83d\udee0\ufe0f</li> <li>Model HUB: Access to a collection of pre-trained models ready for inference \ud83c\udf1f</li> <li>Train/Fine-tune: Easily train or fine-tune your own models \ud83d\udd27</li> <li>Export-Friendly: Export easily to CoreML or TFLite formats \ud83d\udce6</li> </ul>"},{"location":"#quick-installation","title":"Quick Installation","text":"<p>Install for inference:</p> <pre><code>pip install fast-plate-ocr[onnx-gpu]\n</code></pre> <p>Install for training:</p> <pre><code>pip install fast-plate-ocr[train]\n</code></pre> <p>For full installation options (like GPU backends or ONNX variants), see the Installation Guide.</p>"},{"location":"#quick-usage","title":"Quick Usage","text":"<p>Run OCR on a cropped license plate image using <code>LicensePlateRecognizer</code>:</p> <pre><code>from fast_plate_ocr import LicensePlateRecognizer\n\nm = LicensePlateRecognizer(\"cct-xs-v1-global-model\")\nprint(m.run(\"test_plate.png\"))\n</code></pre> <p>For more examples and input formats (NumPy arrays, batches, etc.), see the Inference Guide.</p>"},{"location":"#use-it-with-fastalpr","title":"Use it with FastALPR","text":"<p>If you prefer not to use <code>fast-plate-ocr</code> directly on cropped plates, you can easily leverage it through FastALPR, an end-to-end Automatic License Plate Recognition library where <code>fast-plate-ocr</code> serves as the default OCR backend.</p> <pre><code>from fast_alpr import ALPR  # (1)!\n\nalpr = ALPR(\n    detector_model=\"yolo-v9-t-384-license-plate-end2end\",\n    ocr_model=\"global-plates-mobile-vit-v2-model\",  # (2)!\n)\n\nalpr_results = alpr.predict(\"assets/test_image.png\")\nprint(alpr_results)\n</code></pre> <ol> <li>Requires <code>fast-alpr</code> package to be installed!</li> <li>Can be any of the default <code>fast-plate-ocr</code> trained models or custom ones too!</li> </ol> <p>Explore More</p> <p>Check out the FastALPR docs for full ALPR pipeline and integration tips!</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#inference","title":"Inference","text":"<p>For inference, install:</p> <pre><code>pip install fast-plate-ocr[onnx]\n</code></pre> Warning <p>By default, no ONNX runtime is installed.</p> <p>To run inference, you must install one of the ONNX extras:</p> <ul> <li><code>onnx</code> - for CPU inference (cross-platform)</li> <li><code>onnx-gpu</code> - for NVIDIA GPUs (CUDA)</li> <li><code>onnx-openvino</code> - for Intel CPUs / VPUs</li> <li><code>onnx-directml</code> - for Windows devices via DirectML</li> <li><code>onnx-qnn</code> - for Qualcomm chips on mobile</li> </ul> <p>Dependencies for inference are kept minimal by default. Inference-related packages like ONNX runtimes are optional and not installed unless explicitly requested via extras.</p>"},{"location":"installation/#train","title":"Train","text":"<p>Training code uses Keras 3, which works with multiple backends like TensorFlow, JAX, and PyTorch. You can choose the one that fits your needs, no code changes required.</p> <p>To train or use the CLI tool, you'll need to install:</p> <pre><code>pip install fast-plate-ocr[train]\n</code></pre> Tip <p>You will need to install your desired framework for training as <code>fast-plate-ocr</code> doesn't enforce you to use any specific framework. See Keras backend section.</p> Using TensorFlow with GPU support <p>If you want to use TensorFlow with GPU support as the backend, install it with:</p> <pre><code>pip install tensorflow[and-cuda]\n</code></pre> <p>This ensures that the required CUDA libraries are included. For details, see TensorFlow GPU setup.</p>"},{"location":"contributing/development/","title":"Development","text":"<p>Contributions are greatly appreciated. Whether it's bug fixes, feature enhancements, or new models, your contributions are warmly welcomed.</p> <p>To start contributing or to begin development, you can follow these steps:</p> <ol> <li>Clone repo     <pre><code>git clone https://github.com/ankandrew/fast-plate-ocr.git\n</code></pre></li> <li>Install all dependencies (make sure you have Poetry installed):     <pre><code>make install\n</code></pre></li> <li>To ensure your changes pass linting and tests before submitting a PR:     <pre><code>make checks\n</code></pre></li> </ol> Tip <p>If you want to train a model and share it, we'll add it to the HUB \ud83d\ude80</p>"},{"location":"inference/model_zoo/","title":"Available Models","text":""},{"location":"inference/model_zoo/#model-zoo","title":"Model Zoo","text":"<p>Optimized, ready to use models with config files for inference or fine-tuning.</p> Model Name Size Arch b=1 Avg. Latency (ms) Plates/sec (PPS) Model Config Plate Config <code>cct-s-v1-global-model</code> S CCT 0.5877 1701.63 link link <code>cct-xs-v1-global-model</code> XS CCT 0.3232 3094.21 link link Benchmarking Setup <p>These results were obtained with:</p> <ul> <li>Hardware: NVIDIA RTX 3090 GPU</li> <li>Execution Providers: <code>['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']</code></li> <li>Install dependencies:   <pre><code>pip install fast-plate-ocr[onnx-gpu]\n</code></pre></li> </ul>"},{"location":"inference/model_zoo/#legacy-models","title":"Legacy Models","text":"<p>These are pre-trained models from earlier iterations of <code>fast-plate-ocr</code>, primarily kept for inference purposes.</p> Model Name Time b=1 (ms)<sup>[1]</sup> Throughput  (plates/second)<sup>[1]</sup> Accuracy<sup>[2]</sup> Dataset <code>argentinian-plates-cnn-model</code> 2.1 476 94.05% Non-synthetic, plates up to 2020. Dataset arg_plate_dataset.zip. <code>argentinian-plates-cnn-synth-model</code> 2.1 476 94.19% Plates up to 2020 + synthetic plates. Dataset arg_plate_dataset_plus_synth.zip. <code>european-plates-mobile-vit-v2-model</code> 2.9 344 92.5%<sup>[3]</sup> European plates (from +40 countries, trained on 40k+ plates). <code>global-plates-mobile-vit-v2-model</code> 2.9 344 93.3%<sup>[4]</sup> Worldwide plates (from +65 countries, trained on 85k+ plates). Legacy Notice <p>These are older models maintained for compatibility and inference only. They are not recommended for fine-tuning or continued development. For best results, use the newer models from the Model Zoo.</p> Inference &amp; Evaluation Info <p><sup>[1]</sup> Inference on Mac M1 chip using CPUExecutionProvider. Utilizing CoreMLExecutionProvider accelerates speed by 5x in the CNN models.</p> <p><sup>[2]</sup> Accuracy is what we refer as plate_acc. See metrics section.</p> <p><sup>[3]</sup> For detailed accuracy for each country see results and the corresponding val split used.</p> <p><sup>[4]</sup> For detailed accuracy for each country see results.</p> Reproduce results <p>Calculate Inference Time:</p> <pre><code>pip install fast-plate-ocr[onnx-gpu]\n</code></pre> <pre><code>from fast_plate_ocr import LicensePlateRecognizer\n\nm = LicensePlateRecognizer(\"argentinian-plates-cnn-model\")\nm.benchmark()\n</code></pre> <p>Calculate Model accuracy:</p> <pre><code>pip install fast-plate-ocr[train]\ncurl -LO https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/arg_cnn_ocr_config.yaml\ncurl -LO https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/arg_cnn_ocr.keras\ncurl -LO https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/arg_plate_benchmark.zip\nunzip arg_plate_benchmark.zip\nfast-plate-ocr valid \\\n    -m arg_cnn_ocr.keras \\\n    --config-file arg_cnn_ocr_config.yaml \\\n    --annotations benchmark/annotations.csv\n</code></pre>"},{"location":"inference/running_inference/","title":"Running Inference","text":""},{"location":"inference/running_inference/#inference-guide","title":"Inference Guide","text":"<p>The <code>fast-plate-ocr</code> library performs high-performance license plate recognition using ONNX Runtime for inference.</p> <p>To run inference use the <code>LicensePlateRecognizer</code> class, which supports a wide range of input types:</p> <ul> <li>File paths (str or Path)</li> <li>NumPy arrays representing single images (grayscale or RGB)</li> <li>Lists of paths or NumPy arrays</li> <li>Pre-batched NumPy arrays (4D shape: (N, H, W, C))</li> </ul> <p>The model automatically handles resizing, padding, and format conversion according to its configuration. Predictions can optionally include character-level confidence scores.</p>"},{"location":"inference/running_inference/#predict-a-single-image","title":"Predict a single image","text":"<pre><code>from fast_plate_ocr import LicensePlateRecognizer\n\nplate_recognizer = LicensePlateRecognizer(\"cct-xs-v1-global-model\")\nprint(plate_recognizer.run(\"test_plate.png\"))\n</code></pre> Demo"},{"location":"inference/running_inference/#predict-a-batch-in-memory","title":"Predict a batch in memory","text":"<pre><code>import cv2\nfrom fast_plate_ocr import LicensePlateRecognizer\n\nplate_recognizer = LicensePlateRecognizer(\"cct-xs-v1-global-model\")\nimgs = [cv2.imread(p) for p in [\"plate1.jpg\", \"plate2.jpg\"]]\nres = plate_recognizer.run(imgs)\n</code></pre>"},{"location":"inference/running_inference/#return-confidence-scores","title":"Return confidence scores","text":"<pre><code>from fast_plate_ocr import LicensePlateRecognizer\n\nplate_recognizer = LicensePlateRecognizer(\"cct-xs-v1-global-model\")\nplates, conf = plate_recognizer.run(\"test_plate.png\", return_confidence=True)\n</code></pre>"},{"location":"inference/running_inference/#benchmark-the-model","title":"Benchmark the model","text":"<pre><code>from fast_plate_ocr import LicensePlateRecognizer\n\nm = LicensePlateRecognizer(\"cct-xs-v1-global-model\")\nm.benchmark()\n</code></pre> Demo <p>For a full list of options see Reference.</p>"},{"location":"reference/core/process/","title":"Process","text":"<p>Utility functions for processing model input/output.</p>"},{"location":"reference/core/process/#fast_plate_ocr.core.process.INTERPOLATION_MAP","title":"INTERPOLATION_MAP  <code>module-attribute</code>","text":"<pre><code>INTERPOLATION_MAP: dict[ImageInterpolation, int] = {\n    \"nearest\": INTER_NEAREST,\n    \"linear\": INTER_LINEAR,\n    \"cubic\": INTER_CUBIC,\n    \"area\": INTER_AREA,\n    \"lanczos4\": INTER_LANCZOS4,\n}\n</code></pre> <p>Mapping from interpolation method name to OpenCV constant.</p>"},{"location":"reference/core/process/#fast_plate_ocr.core.process.read_plate_image","title":"read_plate_image","text":"<pre><code>read_plate_image(\n    image_path: PathLike,\n    image_color_mode: ImageColorMode = \"grayscale\",\n) -&gt; ndarray\n</code></pre> <p>Reads an image from disk in the requested colour mode.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>PathLike</code> <p>Path to the image file.</p> required <code>image_color_mode</code> <code>ImageColorMode</code> <p><code>\"grayscale\"</code> for single-channel or <code>\"rgb\"</code> for three-channel colour. Defaults to <code>\"grayscale\"</code>.</p> <code>'grayscale'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The image as a NumPy array. Grayscale images have shape <code>(H, W)</code>, RGB images have shape <code>(H, W, 3)</code>.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the image file does not exist.</p> <code>ValueError</code> <p>If the image cannot be decoded.</p> Source code in <code>fast_plate_ocr/core/process.py</code> <pre><code>def read_plate_image(\n    image_path: PathLike,\n    image_color_mode: ImageColorMode = \"grayscale\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Reads an image from disk in the requested colour mode.\n\n    Args:\n        image_path: Path to the image file.\n        image_color_mode: ``\"grayscale\"`` for single-channel or ``\"rgb\"`` for three-channel\n            colour. Defaults to ``\"grayscale\"``.\n\n    Returns:\n        The image as a NumPy array.\n            Grayscale images have shape ``(H, W)``, RGB images have shape ``(H, W, 3)``.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist.\n        ValueError: If the image cannot be decoded.\n    \"\"\"\n    image_path = str(image_path)\n\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image not found: {image_path}\")\n\n    if image_color_mode == \"rgb\":\n        raw = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        if raw is None:\n            raise ValueError(f\"Failed to decode image: {image_path}\")\n        img = cv2.cvtColor(raw, cv2.COLOR_BGR2RGB)\n    else:\n        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n        if img is None:\n            raise ValueError(f\"Failed to decode image: {image_path}\")\n\n    return img\n</code></pre>"},{"location":"reference/core/process/#fast_plate_ocr.core.process.resize_image","title":"resize_image","text":"<pre><code>resize_image(\n    img: ndarray,\n    img_height: int,\n    img_width: int,\n    image_color_mode: ImageColorMode = \"grayscale\",\n    keep_aspect_ratio: bool = False,\n    interpolation_method: ImageInterpolation = \"linear\",\n    padding_color: PaddingColor = (114, 114, 114),\n) -&gt; ndarray\n</code></pre> <p>Resizes an in-memory image with optional aspect-ratio preservation and padding.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Input image.</p> required <code>img_height</code> <code>int</code> <p>Target image height.</p> required <code>img_width</code> <code>int</code> <p>Target image width.</p> required <code>image_color_mode</code> <code>ImageColorMode</code> <p>Output colour mode, <code>\"grayscale\"</code> or <code>\"rgb\"</code>.</p> <code>'grayscale'</code> <code>keep_aspect_ratio</code> <code>bool</code> <p>If <code>True</code>, maintain the original aspect ratio using letter-box padding. Defaults to <code>False</code>.</p> <code>False</code> <code>interpolation_method</code> <code>ImageInterpolation</code> <p>Interpolation method used for resizing. Defaults to <code>\"linear\"</code>.</p> <code>'linear'</code> <code>padding_color</code> <code>PaddingColor</code> <p>Padding colour (scalar for grayscale, tuple for RGB). Defaults to <code>(114, 114, 114)</code>.</p> <code>(114, 114, 114)</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The resized image with shape <code>(H, W, C)</code> (a channel axis is added for grayscale).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>padding_color</code> length is not 3 for RGB output.</p> Source code in <code>fast_plate_ocr/core/process.py</code> <pre><code>def resize_image(\n    img: np.ndarray,\n    img_height: int,\n    img_width: int,\n    image_color_mode: ImageColorMode = \"grayscale\",\n    keep_aspect_ratio: bool = False,\n    interpolation_method: ImageInterpolation = \"linear\",\n    padding_color: PaddingColor = (114, 114, 114),\n) -&gt; np.ndarray:\n    \"\"\"\n    Resizes an in-memory image with optional aspect-ratio preservation and padding.\n\n    Args:\n        img: Input image.\n        img_height: Target image height.\n        img_width: Target image width.\n        image_color_mode: Output colour mode, ``\"grayscale\"`` or ``\"rgb\"``.\n        keep_aspect_ratio: If ``True``, maintain the original aspect ratio using letter-box\n            padding. Defaults to ``False``.\n        interpolation_method: Interpolation method used for resizing. Defaults to ``\"linear\"``.\n        padding_color: Padding colour (scalar for grayscale, tuple for RGB). Defaults to\n            ``(114, 114, 114)``.\n\n    Returns:\n        The resized image with shape ``(H, W, C)`` (a channel axis is added for grayscale).\n\n    Raises:\n        ValueError: If ``padding_color`` length is not 3 for RGB output.\n    \"\"\"\n    # pylint: disable=too-many-locals\n\n    interpolation = INTERPOLATION_MAP[interpolation_method]\n\n    if not keep_aspect_ratio:\n        img = cv2.resize(img, (img_width, img_height), interpolation=interpolation)\n\n    else:\n        orig_h, orig_w = img.shape[:2]\n        # Scale ratio (new / old) - choose the limiting dimension\n        r = min(img_height / orig_h, img_width / orig_w)\n        # Compute the size of the resized (unpadded) image\n        new_unpad_w, new_unpad_h = round(orig_w * r), round(orig_h * r)\n        # Resize if necessary\n        if (orig_w, orig_h) != (new_unpad_w, new_unpad_h):\n            img = cv2.resize(img, (new_unpad_w, new_unpad_h), interpolation=interpolation)\n        # Padding on each side\n        dw, dh = (img_width - new_unpad_w) / 2, (img_height - new_unpad_h) / 2\n        top, bottom, left, right = (\n            round(dh - 0.1),\n            round(dh + 0.1),\n            round(dw - 0.1),\n            round(dw + 0.1),\n        )\n        border_color: PaddingColor\n        # Ensure padding colour matches channel count\n        if image_color_mode == \"grayscale\":\n            if isinstance(padding_color, tuple):\n                border_color = int(padding_color[0])\n            else:\n                border_color = int(padding_color)\n        elif image_color_mode == \"rgb\":\n            if isinstance(padding_color, tuple):\n                if len(padding_color) != 3:\n                    raise ValueError(\"padding_color must be length-3 for RGB images\")\n                border_color = tuple(int(c) for c in padding_color)  # type: ignore[assignment]\n            else:\n                border_color = (int(padding_color),) * 3\n        img = cv2.copyMakeBorder(\n            img,\n            top,\n            bottom,\n            left,\n            right,\n            borderType=cv2.BORDER_CONSTANT,\n            value=border_color,  # type: ignore[arg-type]\n        )\n    # Add channel axis for gray so output is HxWxC\n    if image_color_mode == \"grayscale\" and img.ndim == 2:\n        img = np.expand_dims(img, axis=-1)\n\n    return img\n</code></pre>"},{"location":"reference/core/process/#fast_plate_ocr.core.process.read_and_resize_plate_image","title":"read_and_resize_plate_image","text":"<pre><code>read_and_resize_plate_image(\n    image_path: PathLike,\n    img_height: int,\n    img_width: int,\n    image_color_mode: ImageColorMode = \"grayscale\",\n    keep_aspect_ratio: bool = False,\n    interpolation_method: ImageInterpolation = \"linear\",\n    padding_color: PaddingColor = (114, 114, 114),\n) -&gt; ndarray\n</code></pre> <p>Reads an image from disk and resizes it for model input.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>PathLike</code> <p>Path to the image.</p> required <code>img_height</code> <code>int</code> <p>Desired output height.</p> required <code>img_width</code> <code>int</code> <p>Desired output width.</p> required <code>image_color_mode</code> <code>ImageColorMode</code> <p><code>\"grayscale\"</code> or <code>\"rgb\"</code>. Defaults to <code>\"grayscale\"</code>.</p> <code>'grayscale'</code> <code>keep_aspect_ratio</code> <code>bool</code> <p>Whether to preserve aspect ratio via letter-boxing. Defaults to <code>False</code>.</p> <code>False</code> <code>interpolation_method</code> <code>ImageInterpolation</code> <p>Interpolation method to use. Defaults to <code>\"linear\"</code>.</p> <code>'linear'</code> <code>padding_color</code> <code>PaddingColor</code> <p>Colour used for padding when aspect ratio is preserved. Defaults to <code>(114, 114, 114)</code>.</p> <code>(114, 114, 114)</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The resized (and possibly padded) image with shape <code>(H, W, C)</code>.</p> Source code in <code>fast_plate_ocr/core/process.py</code> <pre><code>def read_and_resize_plate_image(\n    image_path: PathLike,\n    img_height: int,\n    img_width: int,\n    image_color_mode: ImageColorMode = \"grayscale\",\n    keep_aspect_ratio: bool = False,\n    interpolation_method: ImageInterpolation = \"linear\",\n    padding_color: PaddingColor = (114, 114, 114),\n) -&gt; np.ndarray:\n    \"\"\"\n    Reads an image from disk and resizes it for model input.\n\n    Args:\n        image_path: Path to the image.\n        img_height: Desired output height.\n        img_width: Desired output width.\n        image_color_mode: ``\"grayscale\"`` or ``\"rgb\"``. Defaults to ``\"grayscale\"``.\n        keep_aspect_ratio: Whether to preserve aspect ratio via letter-boxing. Defaults to\n            ``False``.\n        interpolation_method: Interpolation method to use. Defaults to ``\"linear\"``.\n        padding_color: Colour used for padding when aspect ratio is preserved. Defaults to\n            ``(114, 114, 114)``.\n\n    Returns:\n        The resized (and possibly padded) image with shape ``(H, W, C)``.\n    \"\"\"\n    img = read_plate_image(image_path, image_color_mode=image_color_mode)\n    return resize_image(\n        img,\n        img_height,\n        img_width,\n        image_color_mode=image_color_mode,\n        keep_aspect_ratio=keep_aspect_ratio,\n        interpolation_method=interpolation_method,\n        padding_color=padding_color,\n    )\n</code></pre>"},{"location":"reference/core/process/#fast_plate_ocr.core.process.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(images: ndarray) -&gt; ndarray\n</code></pre> <p>Converts image data to the format expected by the model.</p> <p>The model itself handles pixel-value normalisation, so this function only ensures the batch-dimension and dtype are correct.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>ndarray</code> <p>Image or batch of images with shape <code>(H, W, C)</code> or <code>(N, H, W, C)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A NumPy array with shape <code>(N, H, W, C)</code> and dtype <code>uint8</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input does not have 3 or 4 dimensions.</p> Source code in <code>fast_plate_ocr/core/process.py</code> <pre><code>def preprocess_image(images: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts image data to the format expected by the model.\n\n    The model itself handles pixel-value normalisation, so this function only ensures the\n    batch-dimension and dtype are correct.\n\n    Args:\n        images: Image or batch of images with shape ``(H, W, C)`` or ``(N, H, W, C)``.\n\n    Returns:\n        A NumPy array with shape ``(N, H, W, C)`` and dtype ``uint8``.\n\n    Raises:\n        ValueError: If the input does not have 3 or 4 dimensions.\n    \"\"\"\n    # single sample (H, W, C)\n    if images.ndim == 3:\n        images = np.expand_dims(images, axis=0)\n\n    if images.ndim != 4:\n        raise ValueError(\"Expected input of shape (N, H, W, C).\")\n\n    return images.astype(np.uint8)\n</code></pre>"},{"location":"reference/core/process/#fast_plate_ocr.core.process.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(\n    model_output: ndarray,\n    max_plate_slots: int,\n    model_alphabet: str,\n    return_confidence: bool = False,\n) -&gt; tuple[list[str], ndarray] | list[str]\n</code></pre> <p>Decodes model predictions into licence-plate strings.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>ndarray</code> <p>Raw output tensor from the model.</p> required <code>max_plate_slots</code> <code>int</code> <p>Maximum number of character positions.</p> required <code>model_alphabet</code> <code>str</code> <p>Alphabet used by the model.</p> required <code>return_confidence</code> <code>bool</code> <p>If <code>True</code>, also return per-character confidence scores. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[str], ndarray] | list[str]</code> <p>If <code>return_confidence</code> is <code>False</code>: a list of decoded plate strings. If <code>True</code>: a two-tuple <code>(plates, probs)</code> where</p> <ul> <li><code>plates</code> is the list of decoded strings, and</li> <li><code>probs</code> is an array of shape <code>(N, max_plate_slots)</code> with the corresponding   confidence scores.</li> </ul> Source code in <code>fast_plate_ocr/core/process.py</code> <pre><code>def postprocess_output(\n    model_output: np.ndarray,\n    max_plate_slots: int,\n    model_alphabet: str,\n    return_confidence: bool = False,\n) -&gt; tuple[list[str], np.ndarray] | list[str]:\n    \"\"\"\n    Decodes model predictions into licence-plate strings.\n\n    Args:\n        model_output: Raw output tensor from the model.\n        max_plate_slots: Maximum number of character positions.\n        model_alphabet: Alphabet used by the model.\n        return_confidence: If ``True``, also return per-character confidence scores.\n            Defaults to ``False``.\n\n    Returns:\n        If ``return_confidence`` is ``False``: a list of decoded plate strings.\n            If ``True``: a two-tuple ``(plates, probs)`` where\n\n            * ``plates`` is the list of decoded strings, and\n            * ``probs`` is an array of shape ``(N, max_plate_slots)`` with the corresponding\n              confidence scores.\n    \"\"\"\n    predictions = model_output.reshape((-1, max_plate_slots, len(model_alphabet)))\n    prediction_indices = np.argmax(predictions, axis=-1)\n    alphabet_array = np.array(list(model_alphabet))\n    plate_chars = alphabet_array[prediction_indices]\n    plates: list[str] = np.apply_along_axis(\"\".join, 1, plate_chars).tolist()\n    if return_confidence:\n        probs = np.max(predictions, axis=-1)\n        return plates, probs\n    return plates\n</code></pre>"},{"location":"reference/core/types/","title":"Types","text":"<p>Common custom types used across the lib.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.ImageInterpolation","title":"ImageInterpolation  <code>module-attribute</code>","text":"<pre><code>ImageInterpolation: TypeAlias = Literal[\n    \"nearest\", \"linear\", \"cubic\", \"area\", \"lanczos4\"\n]\n</code></pre> <p>Interpolation method used for resizing the input image.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.ImageColorMode","title":"ImageColorMode  <code>module-attribute</code>","text":"<pre><code>ImageColorMode: TypeAlias = Literal['grayscale', 'rgb']\n</code></pre> <p>Input image color mode. Use <code>grayscale</code> for single-channel input or <code>rgb</code> for 3-channel input.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.PaddingColor","title":"PaddingColor  <code>module-attribute</code>","text":"<pre><code>PaddingColor: TypeAlias = tuple[int, int, int] | int\n</code></pre> <p>Padding colour for letterboxing (only used when keeping image aspect ratio).</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.PathLike","title":"PathLike  <code>module-attribute</code>","text":"<pre><code>PathLike: TypeAlias = str | PathLike\n</code></pre> <p>Path-like objects.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.ImgLike","title":"ImgLike  <code>module-attribute</code>","text":"<pre><code>ImgLike: TypeAlias = PathLike | NDArray[uint8]\n</code></pre> <p>Image-like objects, including paths to image files and NumPy arrays of images.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.BatchOrImgLike","title":"BatchOrImgLike  <code>module-attribute</code>","text":"<pre><code>BatchOrImgLike: TypeAlias = ImgLike | Sequence[ImgLike]\n</code></pre> <p>Image-like objects, including paths to image files and NumPy arrays of images, or a batch of images.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.BatchArray","title":"BatchArray  <code>module-attribute</code>","text":"<pre><code>BatchArray: TypeAlias = NDArray[uint8]\n</code></pre> <p>Numpy array of images, representing a batch of images.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.TensorDataFormat","title":"TensorDataFormat  <code>module-attribute</code>","text":"<pre><code>TensorDataFormat: TypeAlias = Literal[\n    \"channels_last\", \"channels_first\"\n]\n</code></pre> <p>Data format of the input tensor. It can be either <code>channels_last</code> or <code>channels_first</code>. <code>channels_last</code> corresponds to inputs with shape <code>(batch, height, width, channels)</code>, while <code>channels_first</code> corresponds to inputs with shape <code>(batch, channels, height, width)</code>.</p>"},{"location":"reference/core/types/#fast_plate_ocr.core.types.KerasDtypes","title":"KerasDtypes  <code>module-attribute</code>","text":"<pre><code>KerasDtypes: TypeAlias = Literal[\n    \"float16\",\n    \"float32\",\n    \"float64\",\n    \"uint8\",\n    \"uint16\",\n    \"uint32\",\n    \"uint64\",\n    \"int8\",\n    \"int16\",\n    \"int32\",\n    \"int64\",\n    \"bfloat16\",\n    \"bool\",\n    \"string\",\n    \"float8_e4m3fn\",\n    \"float8_e5m2\",\n    \"complex64\",\n    \"complex128\",\n]\n</code></pre> <p>Keras data types supported by the library.</p>"},{"location":"reference/core/utils/","title":"Utils","text":"<p>Common utilities used across the package.</p>"},{"location":"reference/core/utils/#fast_plate_ocr.core.utils.log_time_taken","title":"log_time_taken","text":"<pre><code>log_time_taken(process_name: str) -&gt; Iterator[None]\n</code></pre> <p>A concise context manager to time code snippets and log the result.</p> Usage <pre><code>with log_time_taken(\"process_name\"):\n    # Code snippet to be timed\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>process_name</code> <code>str</code> <p>Name of the process being timed.</p> required Source code in <code>fast_plate_ocr/core/utils.py</code> <pre><code>@contextmanager\ndef log_time_taken(process_name: str) -&gt; Iterator[None]:\n    \"\"\"\n    A concise context manager to time code snippets and log the result.\n\n    Usage:\n        ```python\n        with log_time_taken(\"process_name\"):\n            # Code snippet to be timed\n        ```\n\n    Args:\n        process_name: Name of the process being timed.\n    \"\"\"\n    time_start: float = time.perf_counter()\n    try:\n        yield\n    finally:\n        time_end: float = time.perf_counter()\n        time_elapsed: float = time_end - time_start\n        logger = logging.getLogger(__name__)\n        logger.info(\"Computation time of '%s' = %.3fms\", process_name, 1_000 * time_elapsed)\n</code></pre>"},{"location":"reference/core/utils/#fast_plate_ocr.core.utils.measure_time","title":"measure_time","text":"<pre><code>measure_time() -&gt; Iterator[Callable[[], float]]\n</code></pre> <p>A context manager for measuring execution time (in milliseconds) within its code block.</p> Usage <pre><code>with measure_time() as timer:\n    # Code snippet to be timed\nprint(f\"Code took: {timer()} ms\")\n</code></pre> <p>Returns:</p> Type Description <code>Iterator[Callable[[], float]]</code> <p>A function that returns the elapsed time in milliseconds.</p> Source code in <code>fast_plate_ocr/core/utils.py</code> <pre><code>@contextmanager\ndef measure_time() -&gt; Iterator[Callable[[], float]]:\n    \"\"\"\n    A context manager for measuring execution time (in milliseconds) within its code block.\n\n    Usage:\n        ```python\n        with measure_time() as timer:\n            # Code snippet to be timed\n        print(f\"Code took: {timer()} ms\")\n        ```\n\n    Returns:\n        A function that returns the elapsed time in milliseconds.\n    \"\"\"\n    start_time = end_time = time.perf_counter()\n    yield lambda: (end_time - start_time) * 1_000\n    end_time = time.perf_counter()\n</code></pre>"},{"location":"reference/core/utils/#fast_plate_ocr.core.utils.safe_write","title":"safe_write","text":"<pre><code>safe_write(\n    file: str | PathLike[str],\n    mode: str = \"wb\",\n    encoding: str | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[IO]\n</code></pre> <p>Context manager for safe file writing.</p> <p>Opens the specified file for writing and yields a file object. If an exception occurs during writing, the file is removed before raising the exception.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike[str]</code> <p>Path to the file to write.</p> required <code>mode</code> <code>str</code> <p>File open mode (e.g. <code>\"wb\"</code>, <code>\"w\"</code>, etc.). Defaults to <code>\"wb\"</code>.</p> <code>'wb'</code> <code>encoding</code> <code>str | None</code> <p>Encoding to use (for text modes). Ignored in binary mode.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to <code>open()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[IO]</code> <p>A writable file object.</p> Source code in <code>fast_plate_ocr/core/utils.py</code> <pre><code>@contextmanager\ndef safe_write(\n    file: str | os.PathLike[str],\n    mode: str = \"wb\",\n    encoding: str | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[IO]:\n    \"\"\"\n    Context manager for safe file writing.\n\n    Opens the specified file for writing and yields a file object.\n    If an exception occurs during writing, the file is removed before raising the exception.\n\n    Args:\n        file: Path to the file to write.\n        mode: File open mode (e.g. ``\"wb\"``, ``\"w\"``, etc.). Defaults to ``\"wb\"``.\n        encoding: Encoding to use (for text modes). Ignored in binary mode.\n        **kwargs: Additional arguments passed to ``open()``.\n\n    Returns:\n        A writable file object.\n    \"\"\"\n    try:\n        with open(file, mode, encoding=encoding, **kwargs) as f:\n            yield f\n    except Exception as e:\n        Path(file).unlink(missing_ok=True)\n        raise e\n</code></pre>"},{"location":"reference/inference/hub/","title":"HUB","text":"<p>Utilities function used for doing inference with the OCR models.</p>"},{"location":"reference/inference/hub/#fast_plate_ocr.inference.hub.OcrModel","title":"OcrModel  <code>module-attribute</code>","text":"<pre><code>OcrModel = Literal[\n    \"cct-s-v1-global-model\",\n    \"cct-xs-v1-global-model\",\n    \"argentinian-plates-cnn-model\",\n    \"argentinian-plates-cnn-synth-model\",\n    \"european-plates-mobile-vit-v2-model\",\n    \"global-plates-mobile-vit-v2-model\",\n]\n</code></pre> <p>Available OCR models for doing inference.</p>"},{"location":"reference/inference/hub/#fast_plate_ocr.inference.hub.AVAILABLE_ONNX_MODELS","title":"AVAILABLE_ONNX_MODELS  <code>module-attribute</code>","text":"<pre><code>AVAILABLE_ONNX_MODELS: dict[OcrModel, tuple[str, str]] = {\n    \"cct-s-v1-global-model\": (\n        f\"{BASE_URL}/arg-plates/cct_s_v1_global.onnx\",\n        f\"{BASE_URL}/arg-plates/cct_s_v1_global_plate_config.yaml\",\n    ),\n    \"cct-xs-v1-global-model\": (\n        f\"{BASE_URL}/arg-plates/cct_xs_v1_global.onnx\",\n        f\"{BASE_URL}/arg-plates/cct_xs_v1_global_plate_config.yaml\",\n    ),\n    \"argentinian-plates-cnn-model\": (\n        f\"{BASE_URL}/arg-plates/arg_cnn_ocr.onnx\",\n        f\"{BASE_URL}/arg-plates/arg_cnn_ocr_config.yaml\",\n    ),\n    \"argentinian-plates-cnn-synth-model\": (\n        f\"{BASE_URL}/arg-plates/arg_cnn_ocr_synth.onnx\",\n        f\"{BASE_URL}/arg-plates/arg_cnn_ocr_config.yaml\",\n    ),\n    \"european-plates-mobile-vit-v2-model\": (\n        f\"{BASE_URL}/arg-plates/european_mobile_vit_v2_ocr.onnx\",\n        f\"{BASE_URL}/arg-plates/european_mobile_vit_v2_ocr_config.yaml\",\n    ),\n    \"global-plates-mobile-vit-v2-model\": (\n        f\"{BASE_URL}/arg-plates/global_mobile_vit_v2_ocr.onnx\",\n        f\"{BASE_URL}/arg-plates/global_mobile_vit_v2_ocr_config.yaml\",\n    ),\n}\n</code></pre> <p>Dictionary of available OCR models and their URLs.</p>"},{"location":"reference/inference/hub/#fast_plate_ocr.inference.hub.MODEL_CACHE_DIR","title":"MODEL_CACHE_DIR  <code>module-attribute</code>","text":"<pre><code>MODEL_CACHE_DIR: Path = home() / \".cache\" / \"fast-plate-ocr\"\n</code></pre> <p>Default location where models will be stored.</p>"},{"location":"reference/inference/hub/#fast_plate_ocr.inference.hub.download_model","title":"download_model","text":"<pre><code>download_model(\n    model_name: OcrModel,\n    save_directory: Path | None = None,\n    force_download: bool = False,\n) -&gt; tuple[Path, Path]\n</code></pre> <p>Download an OCR model and the config to a given directory.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>OcrModel</code> <p>Which model to download.</p> required <code>save_directory</code> <code>Path | None</code> <p>Directory to save the OCR model. It should point to a folder. If not supplied, this will point to '~/.cache/'. <code>None</code> <code>force_download</code> <code>bool</code> <p>Force and download the model if it already exists in <code>save_directory</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Path, Path]</code> <p>A tuple consisting of (model_downloaded_path, config_downloaded_path).</p> Source code in <code>fast_plate_ocr/inference/hub.py</code> <pre><code>def download_model(\n    model_name: OcrModel,\n    save_directory: pathlib.Path | None = None,\n    force_download: bool = False,\n) -&gt; tuple[pathlib.Path, pathlib.Path]:\n    \"\"\"\n    Download an OCR model and the config to a given directory.\n\n    Args:\n        model_name: Which model to download.\n        save_directory: Directory to save the OCR model. It should point to a folder.\n            If not supplied, this will point to '~/.cache/&lt;model_name&gt;'.\n        force_download: Force and download the model if it already exists in\n            `save_directory`.\n\n    Returns:\n        A tuple consisting of (model_downloaded_path, config_downloaded_path).\n    \"\"\"\n    if model_name not in AVAILABLE_ONNX_MODELS:\n        available_models = \", \".join(AVAILABLE_ONNX_MODELS.keys())\n        raise ValueError(f\"Unknown model {model_name}. Use one of [{available_models}]\")\n\n    if save_directory is None:\n        save_directory = MODEL_CACHE_DIR / model_name\n    elif save_directory.is_file():\n        raise ValueError(f\"Expected a directory, but got {save_directory}\")\n\n    save_directory.mkdir(parents=True, exist_ok=True)\n\n    model_url, plate_config_url = AVAILABLE_ONNX_MODELS[model_name]\n    model_filename = save_directory / model_url.split(\"/\")[-1]\n    plate_config_filename = save_directory / plate_config_url.split(\"/\")[-1]\n\n    if not force_download and model_filename.is_file() and plate_config_filename.is_file():\n        logging.info(\n            \"Skipping download of '%s' model, already exists at %s\",\n            model_name,\n            save_directory,\n        )\n        return model_filename, plate_config_filename\n\n    # Download the model if not present or if we want to force the download\n    if force_download or not model_filename.is_file():\n        logging.info(\"Downloading model to %s\", model_filename)\n        _download_with_progress(url=model_url, filename=model_filename)\n\n    # Same for the config\n    if force_download or not plate_config_filename.is_file():\n        logging.info(\"Downloading config to %s\", plate_config_filename)\n        _download_with_progress(url=plate_config_url, filename=plate_config_filename)\n\n    return model_filename, plate_config_filename\n</code></pre>"},{"location":"reference/inference/inference_class/","title":"LicensePlateRecognizer","text":"<p>ONNX inference class for performing license plates OCR.</p> <p>The current OCR models available from the HUB are:</p> <ul> <li><code>cct-s-v1-global-model</code>: OCR model trained with global plates data. Based on Compact     Convolutional Transformer (CCT) architecture. This is the S variant.</li> <li><code>cct-xs-v1-global-model</code>: OCR model trained with global plates data. Based on Compact     Convolutional Transformer (CCT) architecture. This is the XS variant.</li> <li><code>argentinian-plates-cnn-model</code>: OCR for Argentinian license plates. Uses fully conv     architecture.</li> <li><code>argentinian-plates-cnn-synth-model</code>: OCR for Argentinian license plates trained with     synthetic and real data. Uses fully conv architecture.</li> <li><code>european-plates-mobile-vit-v2-model</code>: OCR for European license plates. Uses     MobileVIT-2 for the backbone.</li> <li><code>global-plates-mobile-vit-v2-model</code>: OCR for global license plates (+65 countries).     Uses MobileVIT-2 for the backbone.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>hub_ocr_model</code> <code>OcrModel | None</code> <p>Name of the OCR model to use from the HUB.</p> <code>None</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>Device type for inference. Should be one of ('cpu', 'cuda', 'auto'). If 'auto' mode, the device will be deduced from <code>onnxruntime.get_available_providers()</code>.</p> <code>'auto'</code> <code>providers</code> <code>Sequence[str | tuple[str, dict]] | None</code> <p>Optional sequence of providers in order of decreasing precedence. If not specified, all available providers are used based on the device argument.</p> <code>None</code> <code>sess_options</code> <code>SessionOptions | None</code> <p>Advanced session options for ONNX Runtime.</p> <code>None</code> <code>onnx_model_path</code> <code>PathLike | None</code> <p>Path to ONNX model file to use (In case you want to use a custom one).</p> <code>None</code> <code>plate_config_path</code> <code>PathLike | None</code> <p>Path to config file to use (In case you want to use a custom one).</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>Force and download the model, even if it already exists.</p> <code>False</code> <p>Returns:     None.</p> Source code in <code>fast_plate_ocr/inference/plate_recognizer.py</code> <pre><code>def __init__(\n    self,\n    hub_ocr_model: OcrModel | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    providers: Sequence[str | tuple[str, dict]] | None = None,\n    sess_options: ort.SessionOptions | None = None,\n    onnx_model_path: PathLike | None = None,\n    plate_config_path: PathLike | None = None,\n    force_download: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes the `LicensePlateRecognizer` with the specified OCR model and inference device.\n\n    The current OCR models available from the HUB are:\n\n    - `cct-s-v1-global-model`: OCR model trained with **global** plates data. Based on Compact\n        Convolutional Transformer (CCT) architecture. This is the **S** variant.\n    - `cct-xs-v1-global-model`: OCR model trained with **global** plates data. Based on Compact\n        Convolutional Transformer (CCT) architecture. This is the **XS** variant.\n    - `argentinian-plates-cnn-model`: OCR for **Argentinian** license plates. Uses fully conv\n        architecture.\n    - `argentinian-plates-cnn-synth-model`: OCR for **Argentinian** license plates trained with\n        synthetic and real data. Uses fully conv architecture.\n    - `european-plates-mobile-vit-v2-model`: OCR for **European** license plates. Uses\n        MobileVIT-2 for the backbone.\n    - `global-plates-mobile-vit-v2-model`: OCR for **global** license plates (+65 countries).\n        Uses MobileVIT-2 for the backbone.\n\n    Args:\n        hub_ocr_model: Name of the OCR model to use from the HUB.\n        device: Device type for inference. Should be one of ('cpu', 'cuda', 'auto'). If\n            'auto' mode, the device will be deduced from\n            `onnxruntime.get_available_providers()`.\n        providers: Optional sequence of providers in order of decreasing precedence. If not\n            specified, all available providers are used based on the device argument.\n        sess_options: Advanced session options for ONNX Runtime.\n        onnx_model_path: Path to ONNX model file to use (In case you want to use a custom one).\n        plate_config_path: Path to config file to use (In case you want to use a custom one).\n        force_download: Force and download the model, even if it already exists.\n    Returns:\n        None.\n    \"\"\"\n    self.logger = logging.getLogger(__name__)\n\n    if providers is not None:\n        self.providers = providers\n        self.logger.info(\"Using custom providers: %s\", providers)\n    else:\n        if device == \"cuda\":\n            self.providers = [\"CUDAExecutionProvider\"]\n        elif device == \"cpu\":\n            self.providers = [\"CPUExecutionProvider\"]\n        elif device == \"auto\":\n            self.providers = ort.get_available_providers()\n        else:\n            raise ValueError(\n                f\"Device should be one of ('cpu', 'cuda', 'auto'). Got '{device}'.\"\n            )\n\n        self.logger.info(\"Using device '%s' with providers: %s\", device, self.providers)\n\n    if onnx_model_path and plate_config_path:\n        onnx_model_path = pathlib.Path(onnx_model_path)\n        plate_config_path = pathlib.Path(plate_config_path)\n        if not onnx_model_path.exists() or not plate_config_path.exists():\n            raise FileNotFoundError(\"Missing model/config file!\")\n        self.model_name = onnx_model_path.stem\n    elif hub_ocr_model:\n        self.model_name = hub_ocr_model\n        onnx_model_path, plate_config_path = hub.download_model(\n            model_name=hub_ocr_model, force_download=force_download\n        )\n    else:\n        raise ValueError(\n            \"Either provide a model from the HUB or a custom model_path and config_path\"\n        )\n\n    self.config = PlateOCRConfig.from_yaml(plate_config_path)\n    self.model = ort.InferenceSession(\n        onnx_model_path, providers=self.providers, sess_options=sess_options\n    )\n</code></pre>"},{"location":"reference/inference/inference_class/#fast_plate_ocr.inference.plate_recognizer.LicensePlateRecognizer.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    n_iter: int = 2500,\n    batch_size: int = 1,\n    include_processing: bool = False,\n    warmup: int = 250,\n) -&gt; None\n</code></pre> <p>Run an inference benchmark and pretty print the results.</p> <p>It reports the following metrics:</p> <ul> <li>Average latency per batch (milliseconds)</li> <li>Throughput in plates / second (PPS), i.e., how many plates the pipeline can process   per second at the chosen <code>batch_size</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n_iter</code> <code>int</code> <p>The number of iterations to run the benchmark. This determines how many times the inference will be executed to compute the average performance metrics.</p> <code>2500</code> <code>batch_size</code> <p>Batch size to use for the benchmark.</p> <code>1</code> <code>include_processing</code> <code>bool</code> <p>Indicates whether the benchmark should include preprocessing and postprocessing times in the measurement.</p> <code>False</code> <code>warmup</code> <code>int</code> <p>Number of warmup iterations to run before the benchmark.</p> <code>250</code> Source code in <code>fast_plate_ocr/inference/plate_recognizer.py</code> <pre><code>def benchmark(\n    self,\n    n_iter: int = 2_500,\n    batch_size: int = 1,\n    include_processing: bool = False,\n    warmup: int = 250,\n) -&gt; None:\n    \"\"\"\n    Run an inference benchmark and pretty print the results.\n\n    It reports the following metrics:\n\n    * **Average latency per batch** (milliseconds)\n    * **Throughput** in *plates / second* (PPS), i.e., how many plates the pipeline can process\n      per second at the chosen ``batch_size``.\n\n    Args:\n        n_iter: The number of iterations to run the benchmark. This determines how many times\n            the inference will be executed to compute the average performance metrics.\n        batch_size : Batch size to use for the benchmark.\n        include_processing: Indicates whether the benchmark should include preprocessing and\n            postprocessing times in the measurement.\n        warmup: Number of warmup iterations to run before the benchmark.\n    \"\"\"\n    x = np.random.randint(\n        0,\n        256,\n        size=(\n            batch_size,\n            self.config.img_height,\n            self.config.img_width,\n            self.config.num_channels,\n        ),\n        dtype=np.uint8,\n    )\n\n    # Warm-up\n    for _ in range(warmup):\n        if include_processing:\n            self.run(x)\n        else:\n            self.model.run(None, {\"input\": x})\n\n    # Timed loop\n    cum_time = 0.0\n    for _ in range(n_iter):\n        with measure_time() as time_taken:\n            if include_processing:\n                self.run(x)\n            else:\n                self.model.run(None, {\"input\": x})\n        cum_time += time_taken()\n\n    avg_time_ms = cum_time / n_iter if n_iter else 0.0\n    pps = (1_000 / avg_time_ms) * batch_size if n_iter else 0.0\n\n    console = Console()\n    model_info = Panel(\n        Text(f\"Model: {self.model_name}\\nProviders: {self.providers}\", style=\"bold green\"),\n        title=\"Model Information\",\n        border_style=\"bright_blue\",\n        expand=False,\n    )\n    console.print(model_info)\n    table = Table(title=f\"Benchmark for '{self.model_name}'\", border_style=\"bright_blue\")\n    table.add_column(\"Metric\", justify=\"center\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"Value\", justify=\"center\", style=\"magenta\")\n\n    table.add_row(\"Batch size\", str(batch_size))\n    table.add_row(\"Warm-up iters\", str(warmup))\n    table.add_row(\"Timed iterations\", str(n_iter))\n    table.add_row(\"Average Time / batch (ms)\", f\"{avg_time_ms:.4f}\")\n    table.add_row(\"Plates per Second (PPS)\", f\"{pps:.4f}\")\n    console.print(table)\n</code></pre>"},{"location":"reference/inference/inference_class/#fast_plate_ocr.inference.plate_recognizer.LicensePlateRecognizer.run","title":"run","text":"<pre><code>run(\n    source: str | list[str] | NDArray | list[NDArray],\n    return_confidence: bool = False,\n) -&gt; tuple[list[str], NDArray] | list[str]\n</code></pre> <p>Performs OCR to recognize license plate characters from an image or a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | list[str] | NDArray | list[NDArray]</code> <p>One or more image inputs, which can be:</p> <ul> <li>A file path (<code>str</code> or <code>PathLike</code>) to an image.</li> <li>A list of file paths.</li> <li>A NumPy array of a single image, with shape (H, W), (H, W, 1) or (H, W, 3).</li> <li>A list of NumPy arrays, each representing an image.</li> <li>A 4D NumPy array of shape (N, H, W, C), ready for inference.</li> </ul> <p>Images will be automatically resized and converted as needed based on the model's configuration (including color mode and aspect ratio settings).</p> required <code>return_confidence</code> <code>bool</code> <p>Whether to return confidence scores along with plate predictions.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[str], NDArray] | list[str]</code> <p>A list of recognized license plates (one per image). If <code>return_confidence</code> is True,</p> <code>tuple[list[str], NDArray] | list[str]</code> <p>also returns a NumPy array of shape <code>(N, plate_slots)</code> containing the confidence scores</p> <code>tuple[list[str], NDArray] | list[str]</code> <p>for each predicted character.</p> Source code in <code>fast_plate_ocr/inference/plate_recognizer.py</code> <pre><code>def run(\n    self,\n    source: str | list[str] | npt.NDArray | list[npt.NDArray],\n    return_confidence: bool = False,\n) -&gt; tuple[list[str], npt.NDArray] | list[str]:\n    \"\"\"\n    Performs OCR to recognize license plate characters from an image or a list of images.\n\n    Args:\n        source: One or more image inputs, which can be:\n\n            - A file path (`str` or `PathLike`) to an image.\n            - A list of file paths.\n            - A NumPy array of a single image, with shape (H, W), (H, W, 1) or (H, W, 3).\n            - A list of NumPy arrays, each representing an image.\n            - A 4D NumPy array of shape (N, H, W, C), ready for inference.\n\n            Images will be automatically resized and converted as needed based on the model's\n            configuration (including color mode and aspect ratio settings).\n\n        return_confidence: Whether to return confidence scores along with plate predictions.\n\n    Returns:\n        A list of recognized license plates (one per image). If `return_confidence` is True,\n        also returns a NumPy array of shape `(N, plate_slots)` containing the confidence scores\n        for each predicted character.\n    \"\"\"\n    x = _load_image_from_source(source, self.config)\n    # Preprocess\n    x = preprocess_image(x)\n    # Run model\n    y: list[npt.NDArray] = self.model.run(None, {\"input\": x})\n    # Postprocess model output\n    return postprocess_output(\n        y[0],\n        self.config.max_plate_slots,\n        self.config.alphabet,\n        return_confidence=return_confidence,\n    )\n</code></pre>"},{"location":"reference/train/model/","title":"Model Schema","text":"<p>Schema definitions for validating supported model architectures and layer blocks.</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.UnitFloat","title":"UnitFloat  <code>module-attribute</code>","text":"<pre><code>UnitFloat: TypeAlias = Annotated[\n    float, Field(ge=0.0, le=1.0)\n]\n</code></pre> <p>A float that must be in range of [0, 1].</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.PaddingTypeStr","title":"PaddingTypeStr  <code>module-attribute</code>","text":"<pre><code>PaddingTypeStr: TypeAlias = Literal['valid', 'same']\n</code></pre> <p>Padding modes supported by Keras convolution and pooling layers.</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.PositiveIntTuple","title":"PositiveIntTuple  <code>module-attribute</code>","text":"<pre><code>PositiveIntTuple: TypeAlias = (\n    PositiveInt | tuple[PositiveInt, PositiveInt]\n)\n</code></pre> <p>A single positive integer or a tuple of two positive integers, usually used for sizes/strides.</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.NormalizationStr","title":"NormalizationStr  <code>module-attribute</code>","text":"<pre><code>NormalizationStr: TypeAlias = Literal[\n    \"layer_norm\", \"rms_norm\", \"dyt\"\n]\n</code></pre> <p>Available normalization layers.</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.ActivationStr","title":"ActivationStr  <code>module-attribute</code>","text":"<pre><code>ActivationStr: TypeAlias = Literal[\n    \"celu\",\n    \"elu\",\n    \"exponential\",\n    \"gelu\",\n    \"glu\",\n    \"hard_shrink\",\n    \"hard_sigmoid\",\n    \"hard_silu\",\n    \"hard_tanh\",\n    \"leaky_relu\",\n    \"linear\",\n    \"log_sigmoid\",\n    \"log_softmax\",\n    \"mish\",\n    \"relu\",\n    \"relu6\",\n    \"selu\",\n    \"sigmoid\",\n    \"silu\",\n    \"soft_shrink\",\n    \"softmax\",\n    \"softplus\",\n    \"softsign\",\n    \"sparse_plus\",\n    \"sparsemax\",\n    \"squareplus\",\n    \"tanh\",\n    \"tanh_shrink\",\n    \"threshold\",\n]\n</code></pre> <p>Supported Keras activation functions.</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.WeightInitializationStr","title":"WeightInitializationStr  <code>module-attribute</code>","text":"<pre><code>WeightInitializationStr: TypeAlias = Literal[\n    \"glorot_normal\",\n    \"glorot_uniform\",\n    \"he_normal\",\n    \"he_uniform\",\n    \"lecun_normal\",\n    \"lecun_uniform\",\n    \"ones\",\n    \"random_normal\",\n    \"random_uniform\",\n    \"truncated_normal\",\n    \"variance_scaling\",\n    \"zeros\",\n]\n</code></pre> <p>Keras weight initialization strategies.</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.LayerConfig","title":"LayerConfig  <code>module-attribute</code>","text":"<pre><code>LayerConfig = Annotated[\n    _Activation\n    | _Conv2D\n    | _CoordConv2D\n    | _DepthwiseConv2D\n    | _SeparableConv2D\n    | _MLP\n    | _MaxBlurPooling2D\n    | _MaxPooling2D\n    | _AveragePooling2D\n    | _ZeroPadding2D\n    | _SqueezeExcite\n    | _BatchNormalization\n    | _Dropout\n    | _SpatialDropout2D\n    | _GaussianNoise\n    | _LayerNorm\n    | _RMSNorm\n    | _DyT,\n    Field(discriminator=\"layer\"),\n]\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.AnyModelConfig","title":"AnyModelConfig  <code>module-attribute</code>","text":"<pre><code>AnyModelConfig = Annotated[\n    CCTModelConfig, Field(discriminator=\"model\")\n]\n</code></pre> <p>Supported model-architecture. New model configs should be added here.</p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Rescaling","title":"_Rescaling","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Rescaling.scale","title":"scale  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scale: float = 1.0 / 255\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Rescaling.offset","title":"offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset: float = 0.0\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Rescaling.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer()\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self):\n    return keras.layers.Rescaling(self.scale, self.offset)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Activation","title":"_Activation","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Activation.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['Activation']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Activation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation: ActivationStr\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Activation.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.Activation(self.activation)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase","title":"_Conv2DBase","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.filters","title":"filters  <code>instance-attribute</code>","text":"<pre><code>filters: PositiveInt\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.kernel_size","title":"kernel_size  <code>instance-attribute</code>","text":"<pre><code>kernel_size: PositiveIntTuple\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.strides","title":"strides  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strides: PositiveIntTuple = 1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.padding","title":"padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding: PaddingTypeStr = 'same'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.activation","title":"activation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activation: ActivationStr = 'relu'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.use_bias","title":"use_bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_bias: bool = True\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.kernel_initializer","title":"kernel_initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kernel_initializer: WeightInitializationStr = 'he_normal'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2DBase.bias_initializer","title":"bias_initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bias_initializer: WeightInitializationStr = 'zeros'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2D","title":"_Conv2D","text":"<p>               Bases: <code>_Conv2DBase</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['Conv2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Conv2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    params = self.model_dump(exclude={\"layer\"})\n    return keras.layers.Conv2D(**params)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CoordConv2D","title":"_CoordConv2D","text":"<p>               Bases: <code>_Conv2DBase</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CoordConv2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['CoordConv2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CoordConv2D.with_r","title":"with_r  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>with_r: bool = False\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CoordConv2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    params = self.model_dump(exclude={\"layer\", \"with_r\"})\n    return CoordConv2D(with_r=self.with_r, **params)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D","title":"_DepthwiseConv2D","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['DepthwiseConv2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.kernel_size","title":"kernel_size  <code>instance-attribute</code>","text":"<pre><code>kernel_size: PositiveIntTuple\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.strides","title":"strides  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strides: PositiveIntTuple = 1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.padding","title":"padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding: PaddingTypeStr = 'same'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.depth_multiplier","title":"depth_multiplier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>depth_multiplier: PositiveInt = 1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.activation","title":"activation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activation: ActivationStr = 'relu'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.use_bias","title":"use_bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_bias: bool = True\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.depthwise_initializer","title":"depthwise_initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>depthwise_initializer: WeightInitializationStr = \"he_normal\"\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.bias_initializer","title":"bias_initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bias_initializer: WeightInitializationStr = 'zeros'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DepthwiseConv2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.DepthwiseConv2D(\n        kernel_size=self.kernel_size,\n        strides=self.strides,\n        padding=self.padding,\n        depth_multiplier=self.depth_multiplier,\n        activation=self.activation,\n        use_bias=self.use_bias,\n        depthwise_initializer=self.depthwise_initializer,\n        bias_initializer=self.bias_initializer,\n    )\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D","title":"_SeparableConv2D","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['SeparableConv2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.filters","title":"filters  <code>instance-attribute</code>","text":"<pre><code>filters: PositiveInt\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.kernel_size","title":"kernel_size  <code>instance-attribute</code>","text":"<pre><code>kernel_size: PositiveIntTuple\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.strides","title":"strides  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strides: PositiveIntTuple = 1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.padding","title":"padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding: PaddingTypeStr = 'same'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.depth_multiplier","title":"depth_multiplier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>depth_multiplier: PositiveInt = 1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.activation","title":"activation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activation: ActivationStr = 'relu'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.use_bias","title":"use_bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_bias: bool = True\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.depthwise_initializer","title":"depthwise_initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>depthwise_initializer: WeightInitializationStr = \"he_normal\"\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.pointwise_initializer","title":"pointwise_initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pointwise_initializer: WeightInitializationStr = (\n    \"glorot_uniform\"\n)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.bias_initializer","title":"bias_initializer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bias_initializer: WeightInitializationStr = 'zeros'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SeparableConv2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.SeparableConv2D(\n        filters=self.filters,\n        kernel_size=self.kernel_size,\n        strides=self.strides,\n        padding=self.padding,\n        depth_multiplier=self.depth_multiplier,\n        activation=self.activation,\n        use_bias=self.use_bias,\n        depthwise_initializer=self.depthwise_initializer,\n        pointwise_initializer=self.pointwise_initializer,\n        bias_initializer=self.bias_initializer,\n    )\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MLP","title":"_MLP","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MLP.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['MLP']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MLP.hidden_units","title":"hidden_units  <code>instance-attribute</code>","text":"<pre><code>hidden_units: list[PositiveInt]\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MLP.dropout_rate","title":"dropout_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout_rate: UnitFloat = 0.1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MLP.activation","title":"activation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activation: ActivationStr = 'gelu'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MLP.use_bias","title":"use_bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_bias: bool = True\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MLP.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return MLP(\n        hidden_units=self.hidden_units,\n        dropout_rate=self.dropout_rate,\n        activation=self.activation,\n        use_bias=self.use_bias,\n    )\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxBlurPooling2D","title":"_MaxBlurPooling2D","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxBlurPooling2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['MaxBlurPooling2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxBlurPooling2D.pool_size","title":"pool_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pool_size: PositiveInt = 2\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxBlurPooling2D.filter_size","title":"filter_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filter_size: PositiveInt = 3\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxBlurPooling2D.padding","title":"padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding: PaddingTypeStr = 'same'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxBlurPooling2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return MaxBlurPooling2D(\n        pool_size=self.pool_size, filter_size=self.filter_size, padding=self.padding\n    )\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxPooling2D","title":"_MaxPooling2D","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxPooling2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['MaxPooling2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxPooling2D.pool_size","title":"pool_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pool_size: PositiveIntTuple = 2\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxPooling2D.strides","title":"strides  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strides: PositiveInt | None = None\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxPooling2D.padding","title":"padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding: PaddingTypeStr = 'valid'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._MaxPooling2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.MaxPooling2D(\n        pool_size=self.pool_size,\n        strides=self.strides,\n        padding=self.padding,\n    )\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._AveragePooling2D","title":"_AveragePooling2D","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._AveragePooling2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['AveragePooling2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._AveragePooling2D.pool_size","title":"pool_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pool_size: PositiveIntTuple = 2\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._AveragePooling2D.strides","title":"strides  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strides: PositiveInt | None = None\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._AveragePooling2D.padding","title":"padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding: PaddingTypeStr = 'valid'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._AveragePooling2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.AveragePooling2D(\n        pool_size=self.pool_size,\n        strides=self.strides,\n        padding=self.padding,\n    )\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._ZeroPadding2D","title":"_ZeroPadding2D","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._ZeroPadding2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['ZeroPadding2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._ZeroPadding2D.padding","title":"padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding: PositiveIntTuple = 1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._ZeroPadding2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.ZeroPadding2D(padding=self.padding)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SqueezeExcite","title":"_SqueezeExcite","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SqueezeExcite.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['SqueezeExcite']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SqueezeExcite.ratio","title":"ratio  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ratio: PositiveFloat = 1.0\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SqueezeExcite.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return SqueezeExcite(ratio=self.ratio)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._BatchNormalization","title":"_BatchNormalization","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._BatchNormalization.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['BatchNormalization']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._BatchNormalization.momentum","title":"momentum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>momentum: PositiveFloat = 0.99\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._BatchNormalization.epsilon","title":"epsilon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>epsilon: PositiveFloat = 0.001\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._BatchNormalization.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: bool = True\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._BatchNormalization.scale","title":"scale  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scale: bool = True\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._BatchNormalization.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.BatchNormalization(\n        momentum=self.momentum,\n        epsilon=self.epsilon,\n        center=self.center,\n        scale=self.scale,\n    )\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Dropout","title":"_Dropout","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Dropout.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['Dropout']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Dropout.rate","title":"rate  <code>instance-attribute</code>","text":"<pre><code>rate: PositiveFloat\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._Dropout.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer()\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self):\n    return keras.layers.Dropout(rate=self.rate)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SpatialDropout2D","title":"_SpatialDropout2D","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SpatialDropout2D.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['SpatialDropout2D']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SpatialDropout2D.rate","title":"rate  <code>instance-attribute</code>","text":"<pre><code>rate: PositiveFloat\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._SpatialDropout2D.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.SpatialDropout2D(rate=self.rate)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._GaussianNoise","title":"_GaussianNoise","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._GaussianNoise.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['GaussianNoise']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._GaussianNoise.stddev","title":"stddev  <code>instance-attribute</code>","text":"<pre><code>stddev: PositiveFloat\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._GaussianNoise.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: int | None = None\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._GaussianNoise.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.GaussianNoise(stddev=self.stddev, seed=self.seed)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._LayerNorm","title":"_LayerNorm","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._LayerNorm.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['LayerNorm']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._LayerNorm.epsilon","title":"epsilon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>epsilon: PositiveFloat = 0.001\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._LayerNorm.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return keras.layers.LayerNormalization(epsilon=self.epsilon)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._RMSNorm","title":"_RMSNorm","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._RMSNorm.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['RMSNorm']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._RMSNorm.epsilon","title":"epsilon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>epsilon: PositiveFloat = 1e-06\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._RMSNorm.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return RMSNormalization(epsilon=self.epsilon)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DyT","title":"_DyT","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DyT.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer: Literal['DyT']\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DyT.alpha_init_value","title":"alpha_init_value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>alpha_init_value: PositiveFloat = 0.5\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._DyT.to_keras_layer","title":"to_keras_layer","text":"<pre><code>to_keras_layer() -&gt; Layer\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def to_keras_layer(self) -&gt; keras.layers.Layer:\n    return DyT(alpha_init_value=self.alpha_init_value)\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTokenizerConfig","title":"_CCTTokenizerConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTokenizerConfig.blocks","title":"blocks  <code>instance-attribute</code>","text":"<pre><code>blocks: list[LayerConfig]\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTokenizerConfig.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: PositiveIntTuple = 1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTokenizerConfig.patch_mlp","title":"patch_mlp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_mlp: _MLP | None = None\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTokenizerConfig.positional_emb","title":"positional_emb  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>positional_emb: bool = True\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig","title":"_CCTTransformerEncoderConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers: PositiveInt\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.heads","title":"heads  <code>instance-attribute</code>","text":"<pre><code>heads: PositiveInt\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.projection_dim","title":"projection_dim  <code>instance-attribute</code>","text":"<pre><code>projection_dim: PositiveInt\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.units","title":"units  <code>instance-attribute</code>","text":"<pre><code>units: list[PositiveInt]\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.activation","title":"activation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activation: ActivationStr = 'gelu'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.stochastic_depth","title":"stochastic_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stochastic_depth: UnitFloat = 0.1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.attention_dropout","title":"attention_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>attention_dropout: UnitFloat = 0.1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.mlp_dropout","title":"mlp_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mlp_dropout: UnitFloat = 0.1\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.head_mlp_dropout","title":"head_mlp_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>head_mlp_dropout: UnitFloat = 0.2\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.token_reducer_heads","title":"token_reducer_heads  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>token_reducer_heads: PositiveInt = 2\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig.normalization","title":"normalization  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalization: NormalizationStr = 'layer_norm'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema._CCTTransformerEncoderConfig._consistency_checks","title":"_consistency_checks","text":"<pre><code>_consistency_checks()\n</code></pre> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>@model_validator(mode=\"after\")\ndef _consistency_checks(self):\n    if self.units[-1] != self.projection_dim:\n        raise ValueError(\n            \"'units[-1]' must equal 'projection_dim' \"\n            f\"(got {self.units[-1]} vs {self.projection_dim}).\"\n        )\n    return self\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.CCTModelConfig","title":"CCTModelConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.CCTModelConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Literal['cct'] = 'cct'\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.CCTModelConfig.rescaling","title":"rescaling  <code>instance-attribute</code>","text":"<pre><code>rescaling: _Rescaling\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.CCTModelConfig.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer: _CCTTokenizerConfig\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.CCTModelConfig.transformer_encoder","title":"transformer_encoder  <code>instance-attribute</code>","text":"<pre><code>transformer_encoder: _CCTTransformerEncoderConfig\n</code></pre>"},{"location":"reference/train/model/#fast_plate_ocr.train.model.model_schema.load_model_config_from_yaml","title":"load_model_config_from_yaml","text":"<pre><code>load_model_config_from_yaml(\n    yaml_path: PathLike,\n) -&gt; AnyModelConfig\n</code></pre> <p>Loads, parses, and validates a YAML file defining a model architecture.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>PathLike</code> <p>Path to the YAML file.</p> required <p>Returns:</p> Name Type Description <code>AnyModelConfig</code> <code>AnyModelConfig</code> <p>Parsed and validated model configuration.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the YAML file does not exist.</p> Source code in <code>fast_plate_ocr/train/model/model_schema.py</code> <pre><code>def load_model_config_from_yaml(yaml_path: PathLike) -&gt; AnyModelConfig:\n    \"\"\"\n    Loads, parses, and validates a YAML file defining a model architecture.\n\n    Args:\n        yaml_path: Path to the YAML file.\n\n    Returns:\n        AnyModelConfig: Parsed and validated model configuration.\n\n    Raises:\n        FileNotFoundError: If the YAML file does not exist.\n    \"\"\"\n    if not Path(yaml_path).is_file():\n        raise FileNotFoundError(f\"Model config '{yaml_path}' doesn't exist.\")\n    with open(yaml_path, encoding=\"utf-8\") as f_in:\n        data = yaml.safe_load(f_in)\n    return AnyModelConfig(**data)\n</code></pre>"},{"location":"reference/train/plate_config/","title":"Plate Config","text":"<p>License Plate OCR config. This config file defines how license plate images and text should be preprocessed for OCR model training and inference.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig","title":"PlateOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model License Plate OCR config.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.max_plate_slots","title":"max_plate_slots  <code>instance-attribute</code>","text":"<pre><code>max_plate_slots: PositiveInt\n</code></pre> <p>Max number of plate slots supported. This represents the number of model classification heads.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.alphabet","title":"alphabet  <code>instance-attribute</code>","text":"<pre><code>alphabet: str\n</code></pre> <p>All the possible character set for the model output.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.pad_char","title":"pad_char  <code>instance-attribute</code>","text":"<pre><code>pad_char: Annotated[\n    str, StringConstraints(min_length=1, max_length=1)\n]\n</code></pre> <p>Padding character for plates which length is smaller than MAX_PLATE_SLOTS.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.img_height","title":"img_height  <code>instance-attribute</code>","text":"<pre><code>img_height: PositiveInt\n</code></pre> <p>Image height which is fed to the model.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.img_width","title":"img_width  <code>instance-attribute</code>","text":"<pre><code>img_width: PositiveInt\n</code></pre> <p>Image width which is fed to the model.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.keep_aspect_ratio","title":"keep_aspect_ratio  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keep_aspect_ratio: bool = False\n</code></pre> <p>Keep aspect ratio of the input image.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.interpolation","title":"interpolation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interpolation: ImageInterpolation = 'linear'\n</code></pre> <p>Interpolation method used for resizing the input image.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.image_color_mode","title":"image_color_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_color_mode: ImageColorMode = 'grayscale'\n</code></pre> <p>Input image color mode. Use 'grayscale' for single-channel input or 'rgb' for 3-channel input.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.PlateOCRConfig.padding_color","title":"padding_color  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding_color: tuple[UInt8, UInt8, UInt8] | UInt8 = (\n    114,\n    114,\n    114,\n)\n</code></pre> <p>Padding color used when keep_aspect_ratio is True. For grayscale images, this should be a single integer and for RGB images, this must be a tuple of three integers.</p>"},{"location":"reference/train/plate_config/#fast_plate_ocr.train.model.config.load_plate_config_from_yaml","title":"load_plate_config_from_yaml","text":"<pre><code>load_plate_config_from_yaml(\n    yaml_path: PathLike,\n) -&gt; PlateOCRConfig\n</code></pre> <p>Reads and parses a YAML file containing the plate configuration.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_path</code> <code>PathLike</code> <p>Path to the YAML file containing the plate config.</p> required <p>Returns:</p> Name Type Description <code>PlateOCRConfig</code> <code>PlateOCRConfig</code> <p>Parsed and validated plate configuration.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the YAML file does not exist.</p> Source code in <code>fast_plate_ocr/train/model/config.py</code> <pre><code>def load_plate_config_from_yaml(yaml_path: PathLike) -&gt; PlateOCRConfig:\n    \"\"\"\n    Reads and parses a YAML file containing the plate configuration.\n\n    Args:\n        yaml_path: Path to the YAML file containing the plate config.\n\n    Returns:\n        PlateOCRConfig: Parsed and validated plate configuration.\n\n    Raises:\n        FileNotFoundError: If the YAML file does not exist.\n    \"\"\"\n    if not Path(yaml_path).is_file():\n        raise FileNotFoundError(f\"Plate config '{yaml_path}' doesn't exist.\")\n    with open(yaml_path, encoding=\"utf-8\") as f_in:\n        yaml_content = yaml.safe_load(f_in)\n    config = PlateOCRConfig(**yaml_content)\n    return config\n</code></pre>"},{"location":"training/backend/","title":"Backend","text":""},{"location":"training/backend/#installation","title":"Installation","text":"<p>Make sure to install the backend framework you want to use:</p> <ul> <li>TensorFlow.</li> <li>JAX.</li> <li>PyTorch.</li> </ul> <p>Keras will automatically use the one selected via <code>KERAS_BACKEND</code>.</p> Example Using TensorFlow with GPU support <p>If you want to use TensorFlow with GPU support as the backend, install it with:</p> <pre><code>pip install tensorflow[and-cuda]\n</code></pre> <p>This ensures that the required CUDA libraries are included. For details, see TensorFlow GPU setup.</p>"},{"location":"training/backend/#keras-backend","title":"Keras Backend","text":"<p>To train the model, you can install the ML Framework you like the most. Keras 3 has support for TensorFlow, JAX and PyTorch backends.</p> <p>To change the Keras backend you can either:</p> <ol> <li>Export <code>KERAS_BACKEND</code> environment variable, i.e. to use JAX for training:     <pre><code>KERAS_BACKEND=tensorflow fast-plate-ocr train --config-file ...\n</code></pre></li> <li>Edit your local config file at <code>~/.keras/keras.json</code>.</li> </ol> Tip <p>Usually training with JAX and TensorFlow is faster.</p>"},{"location":"training/dataset/","title":"Dataset","text":"<p>This page describes the expected format for datasets used when training models with <code>fast-plate-ocr</code>.</p>"},{"location":"training/dataset/#expected-file-format","title":"Expected File Format","text":"<p>Your dataset should be provided as a CSV file with the following structure:</p> Column Name Type Description <code>image_path</code> <code>str</code> Relative path to the license plate image <code>plate_text</code> <code>str</code> Ground-truth text on the plate (no padding) <p>Relative Paths</p> <p>Image paths in the CSV are resolved relative to the location of the CSV file, not the working directory.</p>"},{"location":"training/dataset/#dataset-structure-example","title":"Dataset Structure Example","text":"<p>The dataset should include at least a CSV file (for training and validation) and a folder with corresponding plate images.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 annotations.csv\n\u2502   \u2514\u2500\u2500 images/\n\u2502       \u251c\u2500\u2500 00001.jpg\n\u2502       \u251c\u2500\u2500 00002.jpg\n\u2502       \u251c\u2500\u2500 00003.jpg\n\u2502       \u251c\u2500\u2500 00004.jpg\n\u2502       \u2514\u2500\u2500 00005.jpg\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 annotations.csv\n    \u2514\u2500\u2500 images/\n        \u251c\u2500\u2500 00006.jpg\n        \u2514\u2500\u2500 00007.jpg\n</code></pre> train/annotations.csv<pre><code>image_path,plate_text\nimages/00001.jpg,KNN505\nimages/00002.jpg,J00NCW\nimages/00003.jpg,48593\nimages/00004.jpg,AB123CD\nimages/00005.jpg,17AB\n</code></pre> val/annotations.csv<pre><code>image_path,plate_text\nimages/00006.jpg,NFM374\nimages/00007.jpg,ZXC9871\n</code></pre>"},{"location":"training/intro/","title":"Train Workflow","text":""},{"location":"training/intro/#requirements","title":"Requirements","text":"<p>To train and use the CLI scripts, you'll need to install:</p> <pre><code>pip install fast-plate-ocr[train]\n</code></pre>"},{"location":"training/intro/#prepare-for-training","title":"Prepare for Training","text":"<p>To train the model you will need:</p> <ol> <li>A license plate configuration that defines how license plate images and text should be preprocessed for the OCR model.    For example, the following config is used for plates that have at maximum 9 chars and that are based on the latin-alphabet:     <pre><code># Config for Latin-alphabet plates\n\n# Max number of plate slots supported. This represents the number of model classification heads.\nmax_plate_slots: 9\n# All the possible character set for the model output.\nalphabet: '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ_'\n# Padding character for plates which length is smaller than MAX_PLATE_SLOTS. It should still be present in the alphabet.\npad_char: '_'\n# Image height which is fed to the model.\nimg_height: 64\n# Image width which is fed to the model.\nimg_width: 128\n# Keep the aspect ratio of the input image.\nkeep_aspect_ratio: false\n# Interpolation method used for resizing the input image.\ninterpolation: linear\n# Input image color mode. Use 'grayscale' for single-channel input or 'rgb' for 3-channel input.\nimage_color_mode: rgb\n</code></pre>    See Plate Config section for more details.</li> <li>A model configuration that defines the architecture of the OCR model. You can customize the architecture entirely    via YAML without editing code. See the Model Config section for supported architectures and examples.</li> <li>A labeled dataset, see Dataset section for more info.</li> <li>Run train script:     <pre><code># You can set the backend to either TensorFlow, JAX or PyTorch\n# (just make sure it is installed)\nKERAS_BACKEND=tensorflow fast-plate-ocr train \\\n  --model-config-file models/cct_s_v1.yaml \\\n  --plate-config-file config/latin_plates.yaml \\\n  --annotations data/train.csv \\\n  --val-annotations data/val.csv \\\n  --epochs 150 \\\n  --batch-size 64 \\\n  --output-dir trained_models/\n</code></pre></li> </ol> <p>You will probably want to change the augmentation pipeline to apply to your dataset.</p> <p>In order to do this define an Albumentations pipeline:</p> <pre><code>import albumentations as A\n\ntransform_pipeline = A.Compose(\n    [\n        # ...\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1),\n        A.MotionBlur(blur_limit=(3, 5), p=0.1),\n        A.CoarseDropout(max_holes=10, max_height=4, max_width=4, p=0.3),\n        # ... and any other augmentation ...\n    ]\n)\n\n# Export to a file (this resultant YAML can be used by the train script)\nA.save(transform_pipeline, \"./transform_pipeline.yaml\", data_format=\"yaml\")\n</code></pre> <p>And then you can train using the custom transformation pipeline with the <code>--augmentation-path</code> option.</p>"},{"location":"training/metrics/","title":"Metrics","text":"<p>The following metrics are tracked during model training and validation to help evaluate OCR model performance at both character level and plate level granularity.</p>"},{"location":"training/metrics/#available-metrics","title":"Available Metrics","text":"<p>During training, you will see the following metrics:</p> <ul> <li> <p>plate_acc: Compute the number of license plates that were fully classified. For a single plate, if the   ground truth is <code>ABC123</code> and the prediction is also <code>ABC123</code>, it would score 1. However, if the prediction was   <code>ABD123</code>, it would score 0, as not all characters were correctly classified.</p> </li> <li> <p>cat_acc: Calculate the accuracy of individual characters within the license plates that were   correctly classified. For example, if the correct label is <code>ABC123</code> and the prediction is <code>ABC133</code>, it would yield   a precision of 83.3% (5 out of 6 characters correctly classified), rather than 0% as in plate_acc, because it's not   completely classified correctly.</p> </li> <li> <p>top_3_k: Calculate how frequently the true character is included in the top-3 predictions   (the three predictions with the highest probability).</p> </li> <li> <p>plate_len_acc: Measures how often the predicted length of the license plate matches the ground truth.   For example, if the target plate has 6 characters and the prediction also has 6, it scores 1 (regardless of content).</p> </li> </ul>"},{"location":"training/metrics/#example-cases","title":"Example Cases","text":"Ground Truth Prediction plate_acc char_acc plate_len_acc Notes <code>ABC123</code> <code>ABC123</code> 100% 100% 100% Perfect match <code>ABC123</code> <code>ABD123</code> 0% 83.3% 100% 5 / 6 chars correct <code>XYZ9</code> <code>XYZ9</code> 100% 100% 100% Short plate, all correct <code>XYZ9</code> <code>XYZ99</code> 0% 75.0% 0% Length mismatch + one wrong <code>ABC123</code> <code>ABX1Y3</code> 0% 66.7% 100% Two chars wrong"},{"location":"training/cli/dataset_stats/","title":"Dataset Statistics","text":"<p>The <code>dataset_stats</code> CLI command allows you to show statistics about a dataset prepared for training with <code>fast-plate-ocr</code>.</p> <p>It provides insights into:</p> <ul> <li>Plate text lengths</li> <li>Image dimensions (height, width, aspect ratio)</li> <li>File extensions</li> <li>Most frequent characters</li> </ul>"},{"location":"training/cli/dataset_stats/#usage","title":"Usage","text":"<pre><code>fast-plate-ocr dataset-stats \\\n  --annotations path/to/annotations.csv \\\n  --plate-config-file path/to/plate_config.yaml\n</code></pre> \ud83d\udcca Example Output <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Dataset Statistics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       Plate Lengths                                    \u2502\n\u2502          \u2577      \u2577      \u2577      \u2577      \u2577      \u2577      \u2577                   \u2502\n\u2502    count \u2502 mean \u2502  std \u2502  min \u2502  max \u2502   5% \u2502  50% \u2502  95%              \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550              \u2502\n\u2502  4397.00 \u2502 6.57 \u2502 0.97 \u2502 4.00 \u2502 9.00 \u2502 5.00 \u2502 7.00 \u2502 8.00              \u2502\n\u2502          \u2575      \u2575      \u2575      \u2575      \u2575      \u2575      \u2575                   \u2502\n\u2502                            Image Height                                \u2502\n\u2502          \u2577       \u2577       \u2577       \u2577        \u2577       \u2577       \u2577            \u2502\n\u2502    count \u2502  mean \u2502   std \u2502   min \u2502    max \u2502    5% \u2502   50% \u2502    95%     \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550     \u2502\n\u2502  4397.00 \u2502 76.87 \u2502 36.74 \u2502 16.00 \u2502 673.00 \u2502 32.00 \u2502 73.00 \u2502 133.00     \u2502\n\u2502          \u2575       \u2575       \u2575       \u2575        \u2575       \u2575       \u2575            \u2502\n\u2502                              Image Width                               \u2502\n\u2502          \u2577        \u2577       \u2577       \u2577         \u2577       \u2577        \u2577         \u2502\n\u2502    count \u2502   mean \u2502   std \u2502   min \u2502     max \u2502    5% \u2502    50% \u2502    95%  \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550  \u2502\n\u2502  4397.00 \u2502 190.02 \u2502 95.52 \u2502 42.00 \u2502 1437.00 \u2502 83.00 \u2502 174.00 \u2502 324.00  \u2502\n\u2502          \u2575        \u2575       \u2575       \u2575         \u2575       \u2575        \u2575         \u2502\n\u2502                        Aspect Ratio                                    \u2502\n\u2502          \u2577      \u2577      \u2577      \u2577      \u2577      \u2577      \u2577                   \u2502\n\u2502    count \u2502 mean \u2502  std \u2502  min \u2502  max \u2502   5% \u2502  50% \u2502  95%              \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550              \u2502\n\u2502  4397.00 \u2502 2.62 \u2502 0.86 \u2502 0.57 \u2502 6.48 \u2502 1.24 \u2502 2.65 \u2502 3.90              \u2502\n\u2502          \u2575      \u2575      \u2575      \u2575      \u2575      \u2575      \u2575                   \u2502\n\u2502   Extensions                                                           \u2502\n\u2502       \u2577                                                                \u2502\n\u2502  Ext  \u2502 Count                                                          \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550                                                          \u2502\n\u2502  .jpg \u2502  4397                                                          \u2502\n\u2502       \u2575                                                                \u2502\n\u2502  Top 10 Chars                                                          \u2502\n\u2502       \u2577                                                                \u2502\n\u2502  Char \u2502 Count                                                          \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550                                                          \u2502\n\u2502  1    \u2502  2002                                                          \u2502\n\u2502  0    \u2502  1825                                                          \u2502\n\u2502  2    \u2502  1819                                                          \u2502\n\u2502  3    \u2502  1669                                                          \u2502\n\u2502  7    \u2502  1655                                                          \u2502\n\u2502  9    \u2502  1631                                                          \u2502\n\u2502  5    \u2502  1623                                                          \u2502\n\u2502  4    \u2502  1621                                                          \u2502\n\u2502  6    \u2502  1600                                                          \u2502\n\u2502  8    \u2502  1553                                                          \u2502\n\u2502       \u2575                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/cli/export/","title":"Exporting a Trained OCR Model","text":"<p>The <code>export</code> command converts a trained <code>.keras</code> model into alternative formats like ONNX, TFLite, or CoreML, enabling deployment to different platforms and devices.</p> <p>Inside <code>fast-plate-ocr</code> ecosystem, only ONNX inference is supported. But you are free to export trained models and easily use in any other of the exported formats!</p>"},{"location":"training/cli/export/#export-to-onnx","title":"Export to ONNX","text":""},{"location":"training/cli/export/#basic-usage","title":"Basic Usage","text":"<pre><code>fast-plate-ocr export \\\n  --model trained_models/best.keras \\\n  --plate-config-file config/latin_plates.yaml \\\n  --format onnx\n</code></pre>"},{"location":"training/cli/export/#channels-first-and-input-dtype-float32","title":"Channels first AND input dtype float32","text":"<p>By default, the ONNX models are exported with channels last and input dtype of <code>uint8</code>. There might be cases that you want channels first (<code>BxCxHxW</code>) and input dtype of <code>float32</code>. This is useful for RKNN See this issue for context: fast-plate-ocr/issues/46.</p> <pre><code>fast-plate-ocr export \\\n  --model trained_models/best.keras \\\n  --plate-config-file config/latin_plates.yaml \\\n  --format onnx \\\n  --onnx-data-format channels_first \\\n  --onnx-input-dtype float32\n</code></pre> Model shape compatibility <p>Some formats (like TFLite) only support fixed batch sizes, whereas ONNX allows dynamic batching. The export script handles these differences automatically.</p>"},{"location":"training/cli/export/#export-to-tflite","title":"Export to TFLite","text":"<p>TensorFlow Lite is ideal for deploying models to mobile and edge devices.</p> <pre><code>fast-plate-ocr export \\\n  --model trained_models/best.keras \\\n  --plate-config-file config/latin_plates.yaml \\\n  --format tflite\n</code></pre> TFLite batch dim <p>TFLite does not support dynamic batch sizes, so input is fixed to <code>batch_size=1</code>.</p>"},{"location":"training/cli/export/#export-to-coreml","title":"Export to CoreML","text":"<pre><code>fast-plate-ocr export \\\n  --model trained_models/best.keras \\\n  --plate-config-file config/latin_plates.yaml \\\n  --format coreml\n</code></pre> <p>This will produce a <code>.mlpackage</code> file, compatible with CoreML and Xcode deployments.</p>"},{"location":"training/cli/train/","title":"Training the OCR Model","text":"<p>The <code>train</code> script launches the end-to-end training process for a license plate OCR model, using:</p> <ul> <li>A user-defined model architecture (<code>model_config.yaml</code>)</li> <li>A plate configuration (<code>plate_config.yaml</code>)</li> <li>A training/validation dataset in CSV format</li> <li>Optional augmentation, loss, optimizer and callback settings</li> </ul>"},{"location":"training/cli/train/#basic-usage","title":"Basic Usage","text":"<pre><code># You can set the backend to either TensorFlow, JAX or PyTorch\nKERAS_BACKEND=tensorflow fast-plate-ocr train \\\n  --model-config-file models/cct_s_v1.yaml \\\n  --plate-config-file config/latin_plates.yaml \\\n  --annotations data/train.csv \\\n  --val-annotations data/val.csv \\\n  --epochs 150 \\\n  --batch-size 64 \\\n  --output-dir trained_models/\n</code></pre> Keras Backend <p>Make sure you have installed a backend for training. See Backend section for more details.</p> Tip <p>Use <code>--weights-path &lt;path/to/model.keras&gt;</code> when fine-tuning. All pre-trained models are uploaded to this GH release &gt; Assets.</p>"},{"location":"training/cli/train/#logging-and-checkpoints","title":"Logging and Checkpoints","text":"<p>The script saves:</p> <ul> <li>Best model (based on monitored metric)</li> <li>Final model weights</li> <li>Training parameters and configs</li> <li>Optional TensorBoard logs</li> </ul>"},{"location":"training/cli/train/#metrics-tracked","title":"Metrics Tracked","text":"<p>The model is compiled with:</p> <ul> <li><code>plate_acc</code>: Fully correct plate predictions</li> <li><code>cat_acc</code>: Character-level accuracy</li> <li><code>top_3_k</code>: Accuracy if target is in top-3 predictions</li> <li><code>plate_len_acc</code>: Plate length matches</li> </ul> <p>See Metrics section for more details about metrics.</p> View all CLI flags <p>The CLI supports over 30 options. You can view them with:</p> <pre><code>fast-plate-ocr train --help\n</code></pre>"},{"location":"training/cli/valid/","title":"Validating a Trained OCR Model","text":"<p>The <code>valid</code> command is used to evaluate a trained license plate OCR model on a labeled dataset, using the same configuration and preprocessing setup used during training.</p>"},{"location":"training/cli/valid/#basic-usage","title":"Basic Usage","text":"<pre><code>fast-plate-ocr valid \\\n  --model trained_models/2025-06-28_14-33-51/best.keras \\\n  --plate-config-file config/latin_plates.yaml \\\n  --annotations data/val.csv\n</code></pre>"},{"location":"training/cli/valid/#output","title":"Output","text":"<ul> <li>Evaluation metrics will be printed to the terminal (e.g., accuracy, loss).</li> <li>The script automatically compiles the model using the metrics defined during training.</li> <li>It does not save new weights or modify the model.</li> </ul>"},{"location":"training/cli/valid/#example-output","title":"Example Output","text":"<pre><code>40/40 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 100ms/step - loss: 0.2185 - plate_acc: 0.921 - cat_acc: 0.983 - top_3_k: 0.998\n</code></pre>"},{"location":"training/cli/validate_dataset/","title":"Dataset Validation","text":"<p>Before training your OCR model, it's strongly recommended to validate your dataset using the <code>validate-dataset</code> CLI command. This ensures image integrity, label consistency, and format compatibility with your plate config.</p>"},{"location":"training/cli/validate_dataset/#what-it-checks","title":"What It Checks","text":"<p>The validator performs the following:</p> <ul> <li>Image existence: Verifies that all image paths exist.</li> <li>Image readability: Confirms that images are decodable and not corrupted.</li> <li>Minimum resolution: Flags images smaller than a safe size (i.e., 2x2).</li> <li>Resizing feasibility: Ensures images won't be resized below 1 pixel.</li> <li>Text length: Verifies plate text length are less or equal than <code>max_plate_slots</code>.</li> <li>Alphabet coverage: Ensures all characters are inside the allowed <code>alphabet</code>.</li> <li>Duplicate entries: Warns about repeated image paths.</li> <li>Unused characters: Identifies characters in your <code>alphabet</code> that are not used at all.</li> </ul>"},{"location":"training/cli/validate_dataset/#basic-usage","title":"Basic Usage","text":"<pre><code>fast-plate-ocr validate-dataset \\\n  --annotations-file my-dataset/train.csv \\\n  --plate-config-file config/latin_plates.yaml\n</code></pre>"},{"location":"training/cli/validate_dataset/#fix-and-export-cleaned-file","title":"Fix and Export Cleaned File","text":"<p>To automatically export a cleaned version of your dataset:</p> <pre><code>fast-plate-ocr validate-dataset \\\n  --annotations-file my-dataset/train.csv \\\n  --plate-config-file config/latin_plates.yaml \\\n  --export-fixed train_clean.csv\n</code></pre> <p>This creates <code>train_clean.csv</code> with only valid entries, skipping corrupted rows and malformed labels.</p>"},{"location":"training/cli/validate_dataset/#allow-warnings-but-exit-on-errors","title":"Allow Warnings but Exit on Errors","text":"<p>By default, the validator exits with code <code>1</code> if any error occurs. Use <code>--warn-only</code> to suppress the exit:</p> <pre><code>fast-plate-ocr validate-dataset \\\n  --annotations-file my-dataset/train.csv \\\n  --plate-config-file config/latin_plates.yaml \\\n  --warn-only\n</code></pre>"},{"location":"training/cli/validate_dataset/#control-minimum-resolution","title":"Control Minimum Resolution","text":"<p>Adjust what you consider \"too small\" for images:</p> <pre><code>fast-plate-ocr validate-dataset \\\n  --annotations-file my-dataset/train.csv \\\n  --plate-config-file config/latin_plates.yaml \\\n  --min-height 16 \\\n  --min-width 32\n</code></pre>"},{"location":"training/cli/validate_dataset/#output-example","title":"Output Example","text":"<p>After validation, a summary table is printed to the console using rich formatting:</p> <pre><code> Validation Summary\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Category \u2502 Count \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Errors   \u2502   1   \u2502\n\u2502 Warnings \u2502   1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                      Errors\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Line \u2502 Message                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4554 \u2502 Resize would give 0x0 (0x128)                \u2502\n\u2502      \u2502 from ./img/img_00001.jpg                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                     Warnings\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Line \u2502 Message                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4554 \u2502 Tiny image (1x1437 &lt; 2x2):                   \u2502\n\u2502      \u2502 ./img/img_00001.jpg                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If no errors are found, you're safe to proceed with training.</p>"},{"location":"training/cli/visualize_augmentation/","title":"Visualize Augmentation","text":"<p>Before training, it's helpful to preview the effects of your augmentation pipeline. This <code>visualize-augmentation</code> CLI script lets you quickly visualize how your images will look after transformation, giving you an intuitive sense of whether the augmentations are too strong, too weak, or just right.</p> <p>Image augmentations affect model generalization, so it's important to visually inspect them to ensure they're appropriate and realistic.</p>"},{"location":"training/cli/visualize_augmentation/#example-side-by-side-comparison","title":"Example: Side-by-Side Comparison","text":"<pre><code>fast-plate-ocr visualize-augmentation \\\n  --img-dir benchmark/imgs \\\n  --columns 2 \\\n  --rows 4 \\\n  --show-original \\\n  --augmentation-path transforms.yaml \\\n  --plate-config-file config/latin_plates.yaml\n</code></pre> <p>This shows 8 images (2x4 grid), each combining the original (left) and the augmented (right) version.</p> <p></p>"},{"location":"training/cli/visualize_augmentation/#default-augmentation","title":"Default Augmentation","text":"<p>If <code>--augmentation-path</code> is not provided, the tool uses the default training pipeline from <code>fast_plate_ocr.train.data.augmentation</code>.</p> RGB/Grayscale default augmentation <p>When using <code>rgb</code> in the plate config, the default augmentation applied is slightly different than the one when choosing <code>grayscale</code> (it basically has more color/noise transforms).</p>"},{"location":"training/cli/visualize_predictions/","title":"Visualize Predictions","text":"<p>Once your model is trained, you can visually inspect how it performs on unseen images using:</p> <pre><code>fast-plate-ocr visualize-predictions \\\n  --model arg_cnn_ocr.keras \\\n  --img-dir benchmark/imgs \\\n  --plate-config-file arg_cnn_ocr_config.yaml\n</code></pre>"},{"location":"training/cli/visualize_predictions/#what-youll-see","title":"What You'll See","text":"<p>For each image, the model will:</p> <ul> <li>Predict the license plate text</li> <li>Overlay each predicted character with its confidence score</li> <li>Optionally color low-confidence characters (default: red if below <code>0.35</code>)</li> </ul> <p></p>"},{"location":"training/cli/visualize_predictions/#example-filter-uncertain-plates","title":"Example: Filter Uncertain Plates","text":"<pre><code>fast-plate-ocr visualize-predictions \\\n  --model model.keras \\\n  --img-dir raw_eval_imgs \\\n  --plate-config-file config.yaml \\\n  --filter-conf 0.5\n</code></pre> <p>This filters out predictions unless at least one character has confidence &lt; 0.5, helpful when manually reviewing low-quality predictions.</p>"},{"location":"training/config/model_config/","title":"Model Configuration","text":"<p>The <code>model_config.yaml</code> defines the architecture of the OCR model used in <code>fast-plate-ocr</code>. This configuration allows you to customize key components of the model, i.e., convolutional tokenizers, patching strategies, attention settings, etc. (all without modifying code).</p> <p>All model configurations are validated using Pydantic, ensuring that every field, layer, and parameter is checked when building the model from the config.</p> <p>Currently, the supported architectures are:</p> <ul> <li>CCT (Compact Convolutional Transformer <sup>1</sup>)</li> </ul>"},{"location":"training/config/model_config/#supported-architectures","title":"Supported Architectures","text":""},{"location":"training/config/model_config/#compact-convolutional-transformer-cct","title":"Compact Convolutional Transformer (CCT)","text":"<p>Inspired by the CCT architecture, this model structure:</p> <ol> <li>Uses a convolutional tokenizer to extract patch representations from the input image.</li> <li>Processes the resulting sequence with a Transformer Encoder.</li> </ol>"},{"location":"training/config/model_config/#config-example","title":"Config Example","text":"cct_model_config.yaml<pre><code>model: cct\n\nrescaling:  # (1)!\n  scale: 0.00392156862745098\n  offset: 0.0\n\ntokenizer:\n  blocks:  # (2)!\n    - { layer: Conv2D, filters: 32, kernel_size: 3, strides: 1 }  # (3)!\n    - { layer: MaxBlurPooling2D, pool_size: 2, filter_size: 3 }\n    - { layer: Conv2D, filters: 48, kernel_size: 3, strides: 1 }\n    - { layer: Conv2D, filters: 64, kernel_size: 3, strides: 1 }\n    - { layer: MaxBlurPooling2D, pool_size: 2, filter_size: 3 }\n    - { layer: Conv2D, filters: 80, kernel_size: 3, strides: 1 }\n    - { layer: Conv2D, filters: 96, kernel_size: 3, strides: 1 }\n\n  positional_emb: true\n  patch_size: 2\n  patch_mlp:\n    layer: MLP\n    hidden_units: [64]\n    activation: gelu\n    dropout_rate: 0.05\n\ntransformer_encoder:\n  layers: 4\n  heads: 1\n  projection_dim: 64\n  units: [64, 64]\n  activation: gelu\n  stochastic_depth: 0.05\n  attention_dropout: 0.05\n  mlp_dropout: 0.1\n  head_mlp_dropout: 0.05\n  token_reducer_heads: 4\n  normalization: dyt  # (4)!\n</code></pre> <ol> <li>This scales values between range <code>[0, 1]</code></li> <li>Supports a wide variety of layer types, such as <code>Conv2D</code>, <code>MaxPooling2D</code>, <code>DepthwiseConv2D</code>, <code>SqueezeExcite</code>, etc. See Model Config for all available option.</li> <li>Each layer supports the full set of corresponding Keras parameters. For example, <code>Conv2D</code> accepts <code>filters</code>, <code>kernel_size</code>, <code>strides</code>, etc.</li> <li>See <code>Transformers without Normalization</code> <sup>2</sup></li> </ol> Note on plate/model configs <p>The plate config is used throughout both inference and training scripts. In contrast, the model config (shown above) is only used for training, as it defines the architecture to be built.</p>"},{"location":"training/config/model_config/#building-custom-tokenizers-with-any-keras-layer","title":"Building Custom Tokenizers with Any Keras Layer","text":"<p>You can define your own tokenizer stacks by composing any supported layer like <code>Conv2D</code>, <code>DepthwiseConv2D</code>, <code>SqueezeExcite</code>, and many more directly in YAML, without writing any code.</p> <p>Each layer accepts all its typical Keras parameters, and the model schema is validated with Pydantic, so typos or misconfigured fields are caught immediately.</p> <p>Here's an example with a more diverse set of layers:</p> custom_model_config.yaml<pre><code>tokenizer:\n  blocks:\n    - { layer: Conv2D, filters: 64, kernel_size: 3, activation: relu }\n    - { layer: SqueezeExcite, ratio: 0.5 }\n    - { layer: DepthwiseConv2D, kernel_size: 3, strides: 1 }\n    - { layer: BatchNormalization }\n    - { layer: MaxBlurPooling2D, pool_size: 2, filter_size: 3 }\n    - { layer: Conv2D, filters: 128, kernel_size: 3 }\n    - { layer: CoordConv2D, filters: 96, kernel_size: 3, with_r: true }\n</code></pre> Tip <p>Each <code>layer:</code> value corresponds to a class in the Model Schema, check it out to see all the supported layers and options!</p> <ol> <li> <p>Escaping the Big Data Paradigm with Compact Transformers.\u00a0\u21a9</p> </li> <li> <p>Transformers without Normalization.\u00a0\u21a9</p> </li> </ol>"},{"location":"training/config/plate_config/","title":"Plate Configuration File","text":"<p>The plate config file defines how license plate images and text should be preprocessed for OCR model training and inference.</p> <p>This file is parsed using the <code>PlateOCRConfig</code> class and validated with Pydantic.</p>"},{"location":"training/config/plate_config/#config-fields","title":"Config Fields","text":"<p>Below is a summary of the supported fields in the YAML config:</p> Field Type Description <code>max_plate_slots</code> <code>int</code> Maximum number of characters the model can recognize on a plate <code>alphabet</code> <code>str</code> The full set of characters that the model can output (no duplicates) <code>pad_char</code> <code>str</code> A single character used for padding shorter plate texts <code>img_height</code> <code>int</code> Height of input images <code>img_width</code> <code>int</code> Width of input images <code>keep_aspect_ratio</code> <code>bool</code> Whether to keep the original aspect ratio of the image <code>interpolation</code> <code>\"cubic\"</code>, <code>\"linear\"</code>, etc. Resizing interpolation method <code>image_color_mode</code> <code>\"grayscale\"</code> or <code>\"rgb\"</code> Color mode for input images <code>padding_color</code> <code>int</code> or <code>(int, int, int)</code> Color used to pad the image if aspect ratio is preserved"},{"location":"training/config/plate_config/#config-example","title":"Config Example","text":"plate_config.yaml<pre><code>max_plate_slots: 9\nalphabet: \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ_\"  # (1)!\npad_char: \"_\"  # (2)!\nimg_height: 64\nimg_width: 160\nkeep_aspect_ratio: true\ninterpolation: linear  # (3)!\nimage_color_mode: grayscale\npadding_color: 114  # (4)!\n</code></pre> <ol> <li>All the possible character set for the model output. Must include the pad character.</li> <li>Padding character for plates which length is smaller than MAX_PLATE_SLOTS.</li> <li>Matches OpenCV's interpolation names.</li> <li>Only used when <code>keep_aspect_ratio</code> is True.</li> </ol> Tip <p>For examples used in the default models, checkout config directory (located in root dir of project).</p>"}]}