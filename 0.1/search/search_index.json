{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fast &amp; Lightweight License Plate OCR","text":"<p>FastPlateOCR is a lightweight and fast OCR framework for license plate text recognition. You can train models from scratch or use the trained models for inference.</p> <p>The idea is to use this after a plate object detector, since the OCR expects the cropped plates.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Keras 3 Backend Support: Compatible with TensorFlow, JAX, and PyTorch backends \ud83e\udde0</li> <li>Augmentation Variety: Diverse augmentations via Albumentations library \ud83d\uddbc\ufe0f</li> <li>Efficient Execution: Lightweight models that are cheap to run \ud83d\udcb0</li> <li>ONNX Runtime Inference: Fast and optimized inference with ONNX runtime \u26a1</li> <li>User-Friendly CLI: Simplified CLI for training and validating OCR models \ud83d\udee0\ufe0f</li> <li>Model HUB: Access to a collection of pre-trained models ready for inference \ud83c\udf1f</li> </ul>"},{"location":"#model-zoo","title":"Model Zoo","text":"<p>We currently have the following available models:</p> Model Name Time b=1 (ms)<sup>[1]</sup> Throughput  (plates/second)<sup>[1]</sup> Dataset Accuracy<sup>[2]</sup> Dataset <code>argentinian-plates-cnn-model</code> 2.1 476 arg_plate_dataset.zip 94.05% Non-synthetic, plates up to 2020. <code>argentinian-plates-cnn-synth-model</code> 2.1 476 arg_plate_dataset.zip 94.19% Plates up to 2020 + synthetic plates. <p><sup>[1]</sup> Inference on Mac M1 chip using CPUExecutionProvider. Utilizing CoreMLExecutionProvider accelerates speed by 5x.</p> <p><sup>[2]</sup> Accuracy is what we refer as plate_acc. See metrics section.</p> Reproduce results.  Calculate Inference Time:    <pre><code>pip install fast_plate_ocr  # CPU\n# or\npip install fast_plate_ocr[inference-gpu]  # GPU\n</code></pre> <pre><code>from fast_plate_ocr import ONNXPlateRecognizer\n\nm = ONNXPlateRecognizer(\"argentinian-plates-cnn-model\")\nm.benchmark()\n</code></pre>  Calculate Model accuracy:    <pre><code>pip install fast-plate-ocr[train]\ncurl -LO https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/arg_cnn_ocr_config.yaml\ncurl -LO https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/arg_cnn_ocr.keras\ncurl -LO https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/arg_plate_benchmark.zip\nunzip arg_plate_benchmark.zip\nfast_plate_ocr valid \\\n    -m arg_cnn_ocr.keras \\\n    --config-file arg_cnn_ocr_config.yaml \\\n    --annotations benchmark/annotations.csv\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#convnet-cnn-model","title":"ConvNet (CNN) model","text":"<p>The current model architecture is quite simple but effective. It just consists of a few CNN layers with several output heads. See cnn_ocr_model for implementation details.</p> <p>The model output consists of several heads. Each head represents the prediction of a character of the plate. If the plate consists of 7 characters at most (<code>max_plate_slots=7</code>), then the model would have 7 heads.</p> <p>Example of Argentinian plates:</p> <p></p> <p>Each head will output a probability distribution over the <code>vocabulary</code> specified during training. So the output prediction for a single plate will be of shape <code>(max_plate_slots, vocabulary_size)</code>.</p>"},{"location":"architecture/#model-metrics","title":"Model Metrics","text":"<p>During training, you will see the following metrics</p> <ul> <li> <p>plate_acc: Compute the number of license plates that were fully classified. For a single plate, if the   ground truth is <code>ABC123</code> and the prediction is also <code>ABC123</code>, it would score 1. However, if the prediction was   <code>ABD123</code>, it would score 0, as not all characters were correctly classified.</p> </li> <li> <p>cat_acc: Calculate the accuracy of individual characters within the license plates that were   correctly classified. For example, if the correct label is <code>ABC123</code> and the prediction is <code>ABC133</code>, it would yield   a precision of 83.3% (5 out of 6 characters correctly classified), rather than 0% as in plate_acc, because it's not   completely classified correctly.</p> </li> <li> <p>top_3_k: Calculate how frequently the true character is included in the top-3 predictions   (the three predictions with the highest probability).</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are greatly appreciated. Whether it's bug fixes, feature enhancements, or new models, your contributions are warmly welcomed.</p> <p>To start contributing or to begin development, you can follow these steps:</p> <ol> <li>Clone repo     <pre><code>git clone https://github.com/ankandrew/fast-plate-ocr.git\n</code></pre></li> <li>Install all dependencies using Poetry:     <pre><code>poetry install --all-extras\n</code></pre></li> <li>To ensure your changes pass linting and tests before submitting a PR:     <pre><code>make checks\n</code></pre></li> </ol> Tip <p>If you want to train a model and share it, we'll add it to the HUB \ud83d\ude80</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#inference","title":"Inference","text":"<p>For inference only, install:</p> <pre><code>pip install fast_plate_ocr\n</code></pre> <p>For doing inference on GPU, install:</p> <pre><code>pip install fast_plate_ocr[inference-gpu]\n</code></pre>"},{"location":"installation/#train","title":"Train","text":"<p>To train or use the CLI tool, you'll need to install:</p> <pre><code>pip install fast_plate_ocr[train]\n</code></pre> Info <p>You will probably need to install your desired framework for training. FastPlateOCR doesn't enforce you to use any specific framework. See Keras backend section.</p>"},{"location":"reference/","title":"Reference","text":"<p>ONNX inference module.</p>"},{"location":"reference/#fast_plate_ocr.inference.onnx_inference.ONNXPlateRecognizer","title":"<code>ONNXPlateRecognizer</code>","text":"<p>ONNX inference class for performing license plates OCR.</p> Source code in <code>fast_plate_ocr/inference/onnx_inference.py</code> <pre><code>class ONNXPlateRecognizer:\n    \"\"\"\n    ONNX inference class for performing license plates OCR.\n    \"\"\"\n\n    def __init__(\n        self,\n        hub_ocr_model: OcrModel | None = None,\n        device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n        sess_options: ort.SessionOptions | None = None,\n        model_path: str | os.PathLike[str] | None = None,\n        config_path: str | os.PathLike[str] | None = None,\n        force_download: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the ONNXPlateRecognizer with the specified OCR model and inference device.\n\n        The current OCR models available from the HUB are:\n\n        - `argentinian-plates-cnn-model`: OCR for Argentinian license plates.\n\n        Args:\n            hub_ocr_model: Name of the OCR model to use from the HUB.\n            device: Device type for inference. Should be one of ('cpu', 'cuda', 'auto'). If\n                'auto' mode, the device will be deduced from\n                `onnxruntime.get_available_providers()`.\n            sess_options: Advanced session options for ONNX Runtime.\n            model_path: Path to ONNX model file to use (In case you want to use a custom one).\n            config_path: Path to config file to use (In case you want to use a custom one).\n            force_download: Force and download the model, even if it already exists.\n        Returns:\n            None.\n        \"\"\"\n        self.logger = logging.getLogger(__name__)\n\n        if device == \"cuda\":\n            self.provider = [\"CUDAExecutionProvider\"]\n        elif device == \"cpu\":\n            self.provider = [\"CPUExecutionProvider\"]\n        elif device == \"auto\":\n            self.provider = ort.get_available_providers()\n        else:\n            raise ValueError(f\"Device should be one of ('cpu', 'cuda', 'auto'). Got '{device}'.\")\n\n        if model_path and config_path:\n            model_path = pathlib.Path(model_path)\n            config_path = pathlib.Path(config_path)\n            if not model_path.exists() or not config_path.exists():\n                raise FileNotFoundError(\"Missing model/config file!\")\n            self.model_name = model_path.stem\n        elif hub_ocr_model:\n            self.model_name = hub_ocr_model\n            model_path, config_path = hub.download_model(\n                model_name=hub_ocr_model, force_download=force_download\n            )\n        else:\n            raise ValueError(\n                \"Either provide a model from the HUB or a custom model_path and config_path\"\n            )\n\n        self.config = load_config_from_yaml(config_path)\n        self.model = ort.InferenceSession(\n            model_path, providers=self.provider, sess_options=sess_options\n        )\n        self.logger.info(\"Using ONNX Runtime with %s.\", self.provider[0])\n\n    def benchmark(self, n_iter: int = 10_000, include_processing: bool = False) -&gt; None:\n        \"\"\"\n        Benchmark time taken to run the OCR model. This reports the average inference time and the\n        throughput in plates per second.\n\n        Args:\n            n_iter: The number of iterations to run the benchmark. This determines how many times\n                the inference will be executed to compute the average performance metrics.\n            include_processing: Indicates whether the benchmark should include preprocessing and\n                postprocessing times in the measurement.\n        \"\"\"\n        cum_time = 0.0\n        x = np.random.randint(\n            0, 256, size=(1, self.config[\"img_height\"], self.config[\"img_width\"], 1), dtype=np.uint8\n        )\n        for _ in range(n_iter):\n            with measure_time() as time_taken:\n                if include_processing:\n                    self.run(x)\n                else:\n                    self.model.run(None, {\"input\": x})\n            cum_time += time_taken()\n\n        avg_time = (cum_time / n_iter) if n_iter &gt; 0 else 0.0\n        avg_pps = (1_000 / avg_time) if n_iter &gt; 0 else 0.0\n\n        table = Table(title=f\"Benchmark '{self.model_name}' model\")\n        table.add_column(\"Executor\", justify=\"center\", style=\"cyan\", no_wrap=True)\n        table.add_column(\"Average ms\", style=\"magenta\", justify=\"center\")\n        table.add_column(\"Plates/second\", style=\"magenta\", justify=\"center\")\n        table.add_row(self.provider[0], f\"{avg_time:.4f}\", f\"{avg_pps:.4f}\")\n        console = Console()\n        console.print(table)\n\n    def run(\n        self,\n        source: str | list[str] | npt.NDArray | list[npt.NDArray],\n        return_confidence: bool = False,\n    ) -&gt; tuple[list[str], npt.NDArray] | list[str]:\n        \"\"\"\n        Performs OCR to recognize license plate characters from an image or a list of images.\n\n        Args:\n            source: The path(s) to the image(s), a numpy array representing an image or a list\n                of NumPy arrays. If a numpy array is provided, it is expected to already be in\n                grayscale format, with shape `(H, W) `or `(H, W, 1)`. A list of numpy arrays with\n                different image sizes may also be provided.\n            return_confidence: Whether to return confidence scores along with plate predictions.\n\n        Returns:\n            A list of plates for each input image. If `return_confidence` is True, a numpy\n                array is returned with the shape `(N, plate_slots)`, where N is the batch size and\n                each plate slot is the confidence for the recognized license plate character.\n        \"\"\"\n        x = _load_image_from_source(source)\n        # Preprocess\n        x = preprocess_image(x, self.config[\"img_height\"], self.config[\"img_width\"])\n        # Run model\n        y: list[npt.NDArray] = self.model.run(None, {\"input\": x})\n        # Postprocess model output\n        return postprocess_output(\n            y[0],\n            self.config[\"max_plate_slots\"],\n            self.config[\"alphabet\"],\n            return_confidence=return_confidence,\n        )\n</code></pre>"},{"location":"reference/#fast_plate_ocr.inference.onnx_inference.ONNXPlateRecognizer.__init__","title":"<code>__init__(hub_ocr_model=None, device='auto', sess_options=None, model_path=None, config_path=None, force_download=False)</code>","text":"<p>Initializes the ONNXPlateRecognizer with the specified OCR model and inference device.</p> <p>The current OCR models available from the HUB are:</p> <ul> <li><code>argentinian-plates-cnn-model</code>: OCR for Argentinian license plates.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>hub_ocr_model</code> <code>OcrModel | None</code> <p>Name of the OCR model to use from the HUB.</p> <code>None</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>Device type for inference. Should be one of ('cpu', 'cuda', 'auto'). If 'auto' mode, the device will be deduced from <code>onnxruntime.get_available_providers()</code>.</p> <code>'auto'</code> <code>sess_options</code> <code>SessionOptions | None</code> <p>Advanced session options for ONNX Runtime.</p> <code>None</code> <code>model_path</code> <code>str | PathLike[str] | None</code> <p>Path to ONNX model file to use (In case you want to use a custom one).</p> <code>None</code> <code>config_path</code> <code>str | PathLike[str] | None</code> <p>Path to config file to use (In case you want to use a custom one).</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>Force and download the model, even if it already exists.</p> <code>False</code> <p>Returns:     None.</p> Source code in <code>fast_plate_ocr/inference/onnx_inference.py</code> <pre><code>def __init__(\n    self,\n    hub_ocr_model: OcrModel | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    sess_options: ort.SessionOptions | None = None,\n    model_path: str | os.PathLike[str] | None = None,\n    config_path: str | os.PathLike[str] | None = None,\n    force_download: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initializes the ONNXPlateRecognizer with the specified OCR model and inference device.\n\n    The current OCR models available from the HUB are:\n\n    - `argentinian-plates-cnn-model`: OCR for Argentinian license plates.\n\n    Args:\n        hub_ocr_model: Name of the OCR model to use from the HUB.\n        device: Device type for inference. Should be one of ('cpu', 'cuda', 'auto'). If\n            'auto' mode, the device will be deduced from\n            `onnxruntime.get_available_providers()`.\n        sess_options: Advanced session options for ONNX Runtime.\n        model_path: Path to ONNX model file to use (In case you want to use a custom one).\n        config_path: Path to config file to use (In case you want to use a custom one).\n        force_download: Force and download the model, even if it already exists.\n    Returns:\n        None.\n    \"\"\"\n    self.logger = logging.getLogger(__name__)\n\n    if device == \"cuda\":\n        self.provider = [\"CUDAExecutionProvider\"]\n    elif device == \"cpu\":\n        self.provider = [\"CPUExecutionProvider\"]\n    elif device == \"auto\":\n        self.provider = ort.get_available_providers()\n    else:\n        raise ValueError(f\"Device should be one of ('cpu', 'cuda', 'auto'). Got '{device}'.\")\n\n    if model_path and config_path:\n        model_path = pathlib.Path(model_path)\n        config_path = pathlib.Path(config_path)\n        if not model_path.exists() or not config_path.exists():\n            raise FileNotFoundError(\"Missing model/config file!\")\n        self.model_name = model_path.stem\n    elif hub_ocr_model:\n        self.model_name = hub_ocr_model\n        model_path, config_path = hub.download_model(\n            model_name=hub_ocr_model, force_download=force_download\n        )\n    else:\n        raise ValueError(\n            \"Either provide a model from the HUB or a custom model_path and config_path\"\n        )\n\n    self.config = load_config_from_yaml(config_path)\n    self.model = ort.InferenceSession(\n        model_path, providers=self.provider, sess_options=sess_options\n    )\n    self.logger.info(\"Using ONNX Runtime with %s.\", self.provider[0])\n</code></pre>"},{"location":"reference/#fast_plate_ocr.inference.onnx_inference.ONNXPlateRecognizer.benchmark","title":"<code>benchmark(n_iter=10000, include_processing=False)</code>","text":"<p>Benchmark time taken to run the OCR model. This reports the average inference time and the throughput in plates per second.</p> <p>Parameters:</p> Name Type Description Default <code>n_iter</code> <code>int</code> <p>The number of iterations to run the benchmark. This determines how many times the inference will be executed to compute the average performance metrics.</p> <code>10000</code> <code>include_processing</code> <code>bool</code> <p>Indicates whether the benchmark should include preprocessing and postprocessing times in the measurement.</p> <code>False</code> Source code in <code>fast_plate_ocr/inference/onnx_inference.py</code> <pre><code>def benchmark(self, n_iter: int = 10_000, include_processing: bool = False) -&gt; None:\n    \"\"\"\n    Benchmark time taken to run the OCR model. This reports the average inference time and the\n    throughput in plates per second.\n\n    Args:\n        n_iter: The number of iterations to run the benchmark. This determines how many times\n            the inference will be executed to compute the average performance metrics.\n        include_processing: Indicates whether the benchmark should include preprocessing and\n            postprocessing times in the measurement.\n    \"\"\"\n    cum_time = 0.0\n    x = np.random.randint(\n        0, 256, size=(1, self.config[\"img_height\"], self.config[\"img_width\"], 1), dtype=np.uint8\n    )\n    for _ in range(n_iter):\n        with measure_time() as time_taken:\n            if include_processing:\n                self.run(x)\n            else:\n                self.model.run(None, {\"input\": x})\n        cum_time += time_taken()\n\n    avg_time = (cum_time / n_iter) if n_iter &gt; 0 else 0.0\n    avg_pps = (1_000 / avg_time) if n_iter &gt; 0 else 0.0\n\n    table = Table(title=f\"Benchmark '{self.model_name}' model\")\n    table.add_column(\"Executor\", justify=\"center\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"Average ms\", style=\"magenta\", justify=\"center\")\n    table.add_column(\"Plates/second\", style=\"magenta\", justify=\"center\")\n    table.add_row(self.provider[0], f\"{avg_time:.4f}\", f\"{avg_pps:.4f}\")\n    console = Console()\n    console.print(table)\n</code></pre>"},{"location":"reference/#fast_plate_ocr.inference.onnx_inference.ONNXPlateRecognizer.run","title":"<code>run(source, return_confidence=False)</code>","text":"<p>Performs OCR to recognize license plate characters from an image or a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | list[str] | NDArray | list[NDArray]</code> <p>The path(s) to the image(s), a numpy array representing an image or a list of NumPy arrays. If a numpy array is provided, it is expected to already be in grayscale format, with shape <code>(H, W)</code>or <code>(H, W, 1)</code>. A list of numpy arrays with different image sizes may also be provided.</p> required <code>return_confidence</code> <code>bool</code> <p>Whether to return confidence scores along with plate predictions.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list[str], NDArray] | list[str]</code> <p>A list of plates for each input image. If <code>return_confidence</code> is True, a numpy array is returned with the shape <code>(N, plate_slots)</code>, where N is the batch size and each plate slot is the confidence for the recognized license plate character.</p> Source code in <code>fast_plate_ocr/inference/onnx_inference.py</code> <pre><code>def run(\n    self,\n    source: str | list[str] | npt.NDArray | list[npt.NDArray],\n    return_confidence: bool = False,\n) -&gt; tuple[list[str], npt.NDArray] | list[str]:\n    \"\"\"\n    Performs OCR to recognize license plate characters from an image or a list of images.\n\n    Args:\n        source: The path(s) to the image(s), a numpy array representing an image or a list\n            of NumPy arrays. If a numpy array is provided, it is expected to already be in\n            grayscale format, with shape `(H, W) `or `(H, W, 1)`. A list of numpy arrays with\n            different image sizes may also be provided.\n        return_confidence: Whether to return confidence scores along with plate predictions.\n\n    Returns:\n        A list of plates for each input image. If `return_confidence` is True, a numpy\n            array is returned with the shape `(N, plate_slots)`, where N is the batch size and\n            each plate slot is the confidence for the recognized license plate character.\n    \"\"\"\n    x = _load_image_from_source(source)\n    # Preprocess\n    x = preprocess_image(x, self.config[\"img_height\"], self.config[\"img_width\"])\n    # Run model\n    y: list[npt.NDArray] = self.model.run(None, {\"input\": x})\n    # Postprocess model output\n    return postprocess_output(\n        y[0],\n        self.config[\"max_plate_slots\"],\n        self.config[\"alphabet\"],\n        return_confidence=return_confidence,\n    )\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#api","title":"API","text":"<p>To predict from disk image:</p> <pre><code>from fast_plate_ocr import ONNXPlateRecognizer\n\nm = ONNXPlateRecognizer('argentinian-plates-cnn-model')\nprint(m.run('test_plate.png'))\n</code></pre> Demo <p>To run model benchmark:</p> <pre><code>from fast_plate_ocr import ONNXPlateRecognizer\n\nm = ONNXPlateRecognizer('argentinian-plates-cnn-model')\nm.benchmark()\n</code></pre> Demo <p>For a full list of options see Reference.</p>"},{"location":"usage/#cli","title":"CLI","text":"<p>To train or use the CLI tool, you'll need to install:</p> <pre><code>pip install fast_plate_ocr[train]\n</code></pre>"},{"location":"usage/#train-model","title":"Train Model","text":"<p>To train the model you will need:</p> <ol> <li>A configuration used for the OCR model. Depending on your use case, you might have more plate slots or different set    of characters. Take a look at the config for Argentinian license plate as an example:     <pre><code># Config example for Argentinian License Plates\n# The old license plates contain 6 slots/characters (i.e. JUH697)\n# and new 'Mercosur' contain 7 slots/characters (i.e. AB123CD)\n\n# Max number of plate slots supported. This represents the number of model classification heads.\nmax_plate_slots: 7\n# All the possible character set for the model output.\nalphabet: '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ_'\n# Padding character for plates which length is smaller than MAX_PLATE_SLOTS. It should still be present in the alphabet.\npad_char: '_'\n# Image height which is fed to the model.\nimg_height: 70\n# Image width which is fed to the model.\nimg_width: 140\n</code></pre></li> <li>A labeled dataset,    see arg_plate_dataset.zip    for the expected data format.</li> <li>Run train script:     <pre><code># You can set the backend to either TensorFlow, JAX or PyTorch\n# (just make sure it is installed)\nKERAS_BACKEND=tensorflow fast_plate_ocr train \\\n    --annotations path_to_the_train.csv \\\n    --val-annotations path_to_the_val.csv \\\n    --config-file config.yaml \\\n    --batch-size 128 \\\n    --epochs 750 \\\n    --dense \\\n    --early-stopping-patience 100 \\\n    --reduce-lr-patience 50\n</code></pre></li> </ol> <p>You will probably want to change the augmentation pipeline to apply to your dataset.</p> <p>In order to do this define an Albumentations pipeline:</p> <pre><code>import albumentations as A\n\ntransform_pipeline = A.Compose(\n    [\n        # ...\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1),\n        A.MotionBlur(blur_limit=(3, 5), p=0.1),\n        A.CoarseDropout(max_holes=10, max_height=4, max_width=4, p=0.3),\n        # ... and any other augmentation ...\n    ]\n)\n\n# Export to a file (this resultant YAML can be used by the train script)\nA.save(transform_pipeline, \"./transform_pipeline.yaml\", data_format=\"yaml\")\n</code></pre> <p>And then you can train using the custom transformation pipeline with the <code>--augmentation-path</code> option.</p>"},{"location":"usage/#visualize-augmentation","title":"Visualize Augmentation","text":"<p>It's useful to visualize the augmentation pipeline before training the model. This helps us to identify if we should apply more heavy augmentation or less, as it can hurt the model.</p> <p>You might want to see the augmented image next to the original, to see how much it changed:</p> <pre><code>fast_plate_ocr visualize-augmentation \\\n    --img-dir benchmark/imgs \\\n    --columns 2 \\\n    --show-original \\\n    --augmentation-path '/transform_pipeline.yaml'\n</code></pre> <p>You will see something like:</p> <p></p>"},{"location":"usage/#validate-model","title":"Validate Model","text":"<p>After finishing training you can validate the model on a labeled test dataset.</p> <p>Example:</p> <pre><code>fast_plate_ocr valid \\\n    --model arg_cnn_ocr.keras \\\n    --config-file arg_plate_example.yaml \\\n    --annotations benchmark/annotations.csv\n</code></pre>"},{"location":"usage/#visualize-predictions","title":"Visualize Predictions","text":"<p>Once you finish training your model, you can view the model predictions on raw data with:</p> <pre><code>fast_plate_ocr visualize-predictions \\\n    --model arg_cnn_ocr.keras \\\n    --img-dir benchmark/imgs \\\n    --config-file arg_cnn_ocr_config.yaml\n</code></pre> <p>You will see something like:</p> <p></p>"},{"location":"usage/#export-as-onnx","title":"Export as ONNX","text":"<p>Exporting the Keras model to ONNX format might be beneficial to speed-up inference time.</p> <pre><code>fast_plate_ocr export-onnx \\\n    --model arg_cnn_ocr.keras \\\n    --output-path arg_cnn_ocr.onnx \\\n    --opset 18 \\\n    --config-file arg_cnn_ocr_config.yaml\n</code></pre>"},{"location":"usage/#keras-backend","title":"Keras Backend","text":"<p>To train the model, you can install the ML Framework you like the most. Keras 3 has support for TensorFlow, JAX and PyTorch backends.</p> <p>To change the Keras backend you can either:</p> <ol> <li>Export <code>KERAS_BACKEND</code> environment variable, i.e. to use JAX for training:     <pre><code>KERAS_BACKEND=jax fast_plate_ocr train --config-file ...\n</code></pre></li> <li>Edit your local config file at <code>~/.keras/keras.json</code>.</li> </ol> Tip <p>Usually training with JAX and TensorFlow is faster.</p> <p>Note: You will probably need to install your desired framework for training.</p>"}]}