{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset prep, Fine-tune and model export\n",
        "\n",
        "This notebook demonstrates how to **prepare a dataset**, **fine-tune** a license plate OCR model using `fast-plate-ocr` and **export** the trained model for deployment using the `fast-plate-ocr` ecosystem (**ONNX**) or to other formats like **TFLite** and **CoreML**."
      ],
      "metadata": {
        "id": "62uljHJIwlJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "Let's install `fast-plate-ocr` with `train` and `onnx` extras, as well as `tensorflow` backend for training (**JAX** and **PyTorch** can also be used too for **training**)."
      ],
      "metadata": {
        "id": "ceVmwyHxwp_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6XgHJ2EwW5o"
      },
      "outputs": [],
      "source": [
        "!pip install fast-plate-ocr[train,onnx]\n",
        "!pip install tensorflow[and-cuda]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable TF disable all debugging logs\n",
        "%env TF_CPP_MIN_LOG_LEVEL=3\n",
        "# Use TensorFlow as Keras backned (JAX and PyTorch are also supported!)\n",
        "%env KERAS_BACKEND=tensorflow"
      ],
      "metadata": {
        "id": "vXshQpBTABZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also **download the dataset** we will use for **fine-tuning**. This dataset corresponds to **Colombian** vehicle plates. All the credits of this dataset goes to https://github.com/jdbravo, taken from https://gitlab.com/jdbravo/plates-ocr-train.\n",
        "\n",
        "*Note: This dataset wasn't used to trained the pre-trained models, nor did we use data from Colombia during pre-training the official `fast-plate-ocr` models*"
      ],
      "metadata": {
        "id": "65cOGBlRxPL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The downloaded dataset **already** contains the **format expected** by `fast-plate-ocr`, for details see the [docs](https://ankandrew.github.io/fast-plate-ocr/1.0/training/dataset/) for **creating your own dataset**."
      ],
      "metadata": {
        "id": "3YbJI8_Ey4VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/colombia_dataset_example.zip && unzip -q colombia_dataset_example.zip -d colombia_dataset"
      ],
      "metadata": {
        "id": "dSp1yPQKwiYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, **download** the **keras model** that we will **fine-tune**. Keep in mind, we provide all the `.keras` counterpart to the `.onnx` models used for inference. This way, we can easily **fine-tune** any of the models supported by the lib. For this example, we will **fine-tune** `cct_xs_v1_global` model.\n",
        "\n",
        "TIP: All the models can be found in this [**release**](https://github.com/ankandrew/fast-plate-ocr/releases/tag/arg-plates)."
      ],
      "metadata": {
        "id": "EK6AUALB0aMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the .keras model used for fine-tuning\n",
        "!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/cct_xs_v1_global.keras\n",
        "# Download the plate config. This defines how license plate images and text should be preprocessed for OCR\n",
        "# Although you can modify this to suit your needs, since we will be fine-tuning, we will use the exact same\n",
        "# config that was used originally for training cct_xs_v1_global model\n",
        "!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/cct_xs_v1_global_plate_config.yaml\n",
        "# Download also the model config, since we will fine-tuning above model, we will download the same config that\n",
        "# was used to build and train originally the cct_xs_v1_global model\n",
        "!wget -q https://github.com/ankandrew/fast-plate-ocr/releases/download/arg-plates/cct_xs_v1_global_model_config.yaml"
      ],
      "metadata": {
        "id": "yqWyzYKV1MZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the Dataset\n",
        "\n",
        "Below we will use **scripts** that ship with `fast-plate-ocr` lib, which are immediately **available** after **installation**."
      ],
      "metadata": {
        "id": "0S5Fn8HgyqZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first validate the dataset to see if anything is wrong with it. See more in [docs](https://ankandrew.github.io/fast-plate-ocr/1.0/training/cli/validate_dataset)."
      ],
      "metadata": {
        "id": "W1Uk4VDIzudR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the train annotations\n",
        "!fast-plate-ocr validate-dataset \\\n",
        "  --annotations-file ./colombia_dataset/train_annotations.csv \\\n",
        "  --plate-config-file cct_xs_v1_global_plate_config.yaml"
      ],
      "metadata": {
        "id": "fyM-im3_yuA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the validation annotations\n",
        "!fast-plate-ocr validate-dataset \\\n",
        "  --annotations-file ./colombia_dataset/valid_annotations.csv \\\n",
        "  --plate-config-file cct_xs_v1_global_plate_config.yaml"
      ],
      "metadata": {
        "id": "yKerESqH12g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you see errors when **validating** the dataset, you can use the `--export-fixed annotations_fixed.csv`. This creates annotation `.csv` with only valid entries, skipping corrupted rows and malformed labels.\n",
        "\n",
        "Next let's do a **sanity check** and show statistics about the data that will be used for **training**/**validation**."
      ],
      "metadata": {
        "id": "i41uwSByCm-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fast-plate-ocr dataset-stats \\\n",
        "  --annotations ./colombia_dataset/train_annotations.csv \\\n",
        "  --plate-config-file cct_xs_v1_global_plate_config.yaml"
      ],
      "metadata": {
        "id": "lICow_vl21cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare for Training\n",
        "\n",
        "Now that we have **validated** and **visualized stats** about the dataset, we can start **preparing for training**.\n",
        "\n",
        "Now, we will visualize what the model will **actually see** when training. This is a very important step in the workflow, and it will be crucial to the model ability to generalize and work well."
      ],
      "metadata": {
        "id": "K_8fIfR_v8vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We invoke it this way so we can visualize it properly in this notebook\n",
        "%matplotlib inline\n",
        "%run -m fast_plate_ocr.cli.visualize_augmentation -- \\\n",
        "      --img-dir ./colombia_dataset/train \\\n",
        "      --columns 2 \\\n",
        "      --rows 4 \\\n",
        "      --show-original \\\n",
        "      --num-images 50 \\\n",
        "      --plate-config-file cct_xs_v1_global_plate_config.yaml"
      ],
      "metadata": {
        "id": "ApMG-DMOvhde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **default** data augmentation is applied, but that doesn't mean you can't customize and use your own **augmentation pipeline**. We use [albumentations](https://albumentations.ai), so you can any augmentation available in that lib (there are a lot!).\n",
        "\n",
        "Let's **write** a new **augmentation pipeline**, which will be later used when **training** the model."
      ],
      "metadata": {
        "id": "7TQjKMh22o9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "\n",
        "A.save(\n",
        "    A.Compose(\n",
        "        [\n",
        "            A.Affine(\n",
        "                translate_percent=(-0.02, 0.02),\n",
        "                scale=(0.75, 1.10),\n",
        "                rotate=(-15, 15),\n",
        "                border_mode=cv2.BORDER_CONSTANT,\n",
        "                fill=(0, 0, 0),\n",
        "                shear=(0.0, 0.0),\n",
        "                p=0.75,\n",
        "            ),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.10, contrast_limit=0.10, p=0.5),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.HueSaturationValue(\n",
        "                        hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=10, p=0.7\n",
        "                    ),\n",
        "                    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.3),\n",
        "                ],\n",
        "                p=0.3,\n",
        "            ),\n",
        "            A.RandomGamma(gamma_limit=(95, 105), p=0.20),\n",
        "            A.ToGray(p=0.05),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.GaussianBlur(sigma_limit=(0.2, 0.5), p=0.5),\n",
        "                    A.MotionBlur(blur_limit=(3, 3), p=0.5),\n",
        "                ],\n",
        "                p=0.2,\n",
        "            ),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.GaussNoise(std_range=(0.01, 0.03), p=0.2),\n",
        "                    A.MultiplicativeNoise(multiplier=(0.98, 1.02), p=0.1),\n",
        "                    A.ISONoise(intensity=(0.005, 0.02), p=0.1),\n",
        "                    A.ImageCompression(quality_range=(55, 90), p=0.1),\n",
        "                ],\n",
        "                p=0.3,\n",
        "            ),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.CoarseDropout(\n",
        "                        num_holes_range=(1, 14),\n",
        "                        hole_height_range=(1, 5),\n",
        "                        hole_width_range=(1, 5),\n",
        "                        p=0.2,\n",
        "                    ),\n",
        "                    A.PixelDropout(dropout_prob=0.02, p=0.3),\n",
        "                    A.GridDropout(ratio=0.3, fill=\"random\", p=0.3),\n",
        "                ],\n",
        "                p=0.5,\n",
        "            ),\n",
        "        ]\n",
        "    ),\n",
        "    filepath_or_buffer=\"custom_augmentation.yaml\",\n",
        "    data_format=\"yaml\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "eevV9qGJwrF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to **explore** and play with different **augmentations** even for better results!\n",
        "\n",
        "The best way to validate the augmentation pipeline is to actually **visualize** the **results** applied to our training images. For that you can try the newly created pipeline in the `visualize_augmentation` script (used above) with the `--augmentation-path` pointing to the newly created **augmentation pipeline**.\n",
        "\n",
        "Great, now we have our `custom_augmentation.yaml` that we can later use with the **training** script ðŸš€."
      ],
      "metadata": {
        "id": "OV6U7NiY4fRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model\n",
        "\n",
        "Great, now we are **ready** to **fine-tune** the **model** on the Colombian plates dataset!\n",
        "\n",
        "Before running the train script, let's see how the **pre-trained** model performs on the Colombian dataset, so we have a baseline to compare with, after we fine-tune it. Note that **not a single** Colombian **plate** was used **to train** originally the **pre-trained** model!"
      ],
      "metadata": {
        "id": "Pyt6yGZo3vws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fast-plate-ocr valid \\\n",
        "  --model ./cct_xs_v1_global.keras \\\n",
        "  --plate-config-file ./cct_xs_v1_global_plate_config.yaml \\\n",
        "  --annotations ./colombia_dataset/valid_annotations.csv"
      ],
      "metadata": {
        "id": "VwvtPN2-4XHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad! We can see a `plate_acc: 0.8881`, which means that roughly an **88.8%** of plates were **correctly** classified.\n",
        "\n",
        "Keep in mind `plate_acc` is a **strict** metric, it computes the number of license plates that were fully classified. For a single plate, if the ground truth is ABC123 and the prediction is also ABC123, it would **score 1**. However, if the prediction was ABD123, it would **score 0**, as **not all characters** were correctly classified.\n",
        "\n",
        "See more in [Metrics](https://ankandrew.github.io/fast-plate-ocr/1.0/training/metrics/) for full details."
      ],
      "metadata": {
        "id": "f0cDkDmY4_qX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's **improve** that number by **fine-tuning** the **model**!"
      ],
      "metadata": {
        "id": "F5ixeMsJ6Rnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fast-plate-ocr train \\\n",
        "  --model-config-file ./cct_xs_v1_global_model_config.yaml \\\n",
        "  --plate-config-file ./cct_xs_v1_global_plate_config.yaml \\\n",
        "  --annotations ./colombia_dataset/train_annotations.csv \\\n",
        "  --val-annotations ./colombia_dataset/valid_annotations.csv \\\n",
        "  --augmentation-path custom_augmentation.yaml \\\n",
        "  --epochs 30 \\\n",
        "  --batch-size 32 \\\n",
        "  --output-dir trained_models/ \\\n",
        "  --weights-path cct_xs_v1_global.keras \\\n",
        "  --label-smoothing 0.0 \\\n",
        "  --weight-decay 0.0005 \\\n",
        "  --lr 0.0005"
      ],
      "metadata": {
        "id": "ScdAOSmi57DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now see a val `plate_acc` of `0.97917`, meaning almost **98%** of the plates from the validation set were **correctly classified**! That's roughly **10+%** **comparing** with the **baseline** ðŸŽ‰.\n",
        "\n",
        "*Note: you might sligthly different results depending on your run, but it should match more or less these numbers.*"
      ],
      "metadata": {
        "id": "NTtzob7UESYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export the model\n",
        "\n",
        "Now that we've trained the model, we can **export** it to **ONNX** to use it within `fast-plate-ocr` ecosystem or export it to **other formats** (i.e. **TFLite**, **CoreML**, etc.)."
      ],
      "metadata": {
        "id": "Hxayl08xGq-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_keras_model = \"/content/trained_models/2025-07-06_15-29-13/ckpt-epoch_25-acc_0.981.keras\"   # <--- Make sure to change this, yours will be different\n",
        "exported_onnx = best_keras_model.replace(\".keras\", \".onnx\")\n",
        "!fast-plate-ocr export \\\n",
        "  --format onnx \\\n",
        "  --plate-config-file ./cct_xs_v1_global_plate_config.yaml \\\n",
        "  --simplify \\\n",
        "  --model {best_keras_model}"
      ],
      "metadata": {
        "id": "V5cxjNG5GRTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For exporting the newly trained model to other formats, checkout the [docs](https://ankandrew.github.io/fast-plate-ocr/1.0/training/cli/export/)."
      ],
      "metadata": {
        "id": "vU8xMuaoKO-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Inference\n",
        "\n",
        "Now we have the **ONNX** model **ready** to use it with [LicensePlateRecognizer](https://ankandrew.github.io/fast-plate-ocr/1.0/reference/inference/inference_class/) class! Doing **inference** with it is as simple as writing very few lines of code."
      ],
      "metadata": {
        "id": "SzPEPI2RJGOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fast_plate_ocr import LicensePlateRecognizer\n",
        "\n",
        "plate_recognizer = LicensePlateRecognizer(\n",
        "    onnx_model_path=exported_onnx,\n",
        "    plate_config_path=\"cct_xs_v1_global_plate_config.yaml\",\n",
        ")"
      ],
      "metadata": {
        "id": "zzJt4F9BIqFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run inference we can simply call the `.run(...)` method, but **remember** `fast-plate-ocr` **expects** the **cropped plate**.\n",
        "\n",
        "To **use** the **trained model** with an actual **plate detector** (which **localizes** and **crops** the plate) into the expected format expect by `fast-plate-ocr`, we can easily use out newly trained and exported model with [**FastALPR**](https://github.com/ankandrew/fast-alpr).\n"
      ],
      "metadata": {
        "id": "83YOeB9URRP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, install fast-alpr:"
      ],
      "metadata": {
        "id": "J0z7QNFUSDyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fast-alpr[onnx]  # or fast-alpr[onnx-gpu] for GPU support!"
      ],
      "metadata": {
        "id": "RLhoxneASIKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily integrate our custom ONNX model (trained on the Colombian dataset), with the following:"
      ],
      "metadata": {
        "id": "zNZjhiqXSduC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from fast_alpr import ALPR\n",
        "\n",
        "# Initialize the ALPR\n",
        "alpr = ALPR(\n",
        "    detector_model=\"yolo-v9-t-384-license-plate-end2end\",\n",
        "    ocr_model_path=exported_onnx,\n",
        "    ocr_config_path=\"cct_xs_v1_global_plate_config.yaml\",\n",
        ")"
      ],
      "metadata": {
        "id": "ANnsQFGDSdLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more details and options of the ALPR class in the FastALPR [docs](https://ankandrew.github.io/fast-alpr/latest/)"
      ],
      "metadata": {
        "id": "3TiOgnk_WKPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it with a random image grabbed from the web:"
      ],
      "metadata": {
        "id": "P2L-TYODKAFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/7/71/2020_Renault_Logan_Intens_%28Colombia%29_front_view_02.png \\\n",
        "  -O test_plate.png"
      ],
      "metadata": {
        "id": "tJ7tiW7eKsau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the image\n",
        "image_path = \"test_plate.png\"\n",
        "frame = cv2.imread(image_path)\n",
        "\n",
        "# Draw predictions on the image\n",
        "annotated_frame = alpr.draw_predictions(frame)\n",
        "\n",
        "# Display the result\n",
        "cv2_imshow(annotated_frame)"
      ],
      "metadata": {
        "id": "jZlkgQu7K9IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! You have your own plate recognition, that you can use with a couple lines of code ðŸš€."
      ],
      "metadata": {
        "id": "uRNf7iVjUPL9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVSv5LETWIWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}